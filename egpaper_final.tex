\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage[latin9]{inputenc}
\pagestyle{empty}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=false,
 breaklinks=true,pdfborder={0 0 1},backref=section,colorlinks=false]{hyperref}
\hypersetup{pagebackref=false,letterpaper=true,colorlinks}
\usepackage{breakurl}
\usepackage{dsfont}
%\usepackage{amsmath}
\newtheorem{thm}{\textbf{Theorem}}
\usepackage{authblk}
\usepackage{pbox}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\parallelsum}{\mathbin{\!/\mkern-5mu/\!}}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}
\interfootnotelinepenalty=10000

%%%%%%%%% TITLE
\title{MAP Visibility Estimation for Large-Scale Dynamic 3D Reconstruction\thanks{
\scriptsize\url{http://www.cs.cmu.edu/\~hanbyulj/14/visibility.html}}}

\author{Hanbyul Joo~~~~~~~~~~~~~~~Hyun Soo Park~~~~~~~~~~~~~~~Yaser Sheikh\\
Carnegie Mellon University\\
{\small \tt \{hanbyulj,hyunsoop,yaser\}@cs.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``\sqrt{}}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

\begin{abstract}
Many traditional challenges in reconstructing 3D motion, such as matching across wide baselines and handling occlusion, reduce in significance as the number of unique viewpoints increases. However, to obtain this benefit, a new challenge arises: estimating precisely which cameras observe which points at each instant in time. We present a maximum a posteriori (MAP) estimate of the time-varying visibility of the target points to reconstruct the 3D motion of an event from a large number of cameras. Our algorithm takes, as input, camera poses and image sequences, and outputs the time-varying set of the cameras in which a target patch is visible and its reconstructed trajectory. We model visibility estimation as a MAP estimate by incorporating various cues including photometric consistency, motion consistency, and geometric consistency, in conjunction with a prior that rewards consistent visibilities in proximal cameras. An optimal estimate of visibility is obtained by finding the minimum cut of a capacitated graph over cameras. We demonstrate that our method estimates visibility with greater accuracy, and increases tracking performance producing longer trajectories, at more locations, and at higher accuracies than methods that ignore visibility or use photometric consistency alone.
%We present an algorithm to reconstruct the 3D motion of an event from a large number of videos, by explicitly estimating the time-varying visibility of each 3D point. Our algorithm takes, as input, camera poses and image sequences, and outputs the optimal set of the visible cameras and the reconstructed 3D trajectories. We reconstruct the patch motion (location and normal) by triangulating image flow in each camera within a RANSAC framework. The obtained patch motion enables us to define the likelihood of visibility in each camera, based on the geometric consistency of motion and the local similarity of appearance. In this work, we first propose a new cue called motion consistency for the visibility estimation. And, in the end, we fuse all the available information (photometric consistency, motion consistency, and patch normal directional constraint), in conjunction with a Markov Random Field model that rewards consistent visibilities in proximal cameras. An optimal estimate of visibility is obtained by finding the minimum cut in a graph over cameras. We demonstrate that as the number of cameras increases, our algorithm produces longer trajectories, at more locations, and at higher accuracies than methods than ignore visibility or use appearance cues alone.
\end{abstract}

\section{Introduction}
Thousands of images exist for most significant landmarks around the world. The availability of such imagery has facilitated the development of large-scale 3D reconstruction algorithms, which fully leverage the number of views to produce dense and accurate 3D point clouds~\cite{Snavely:2006,Frahm:2010,Furukawa:2010}. Increasingly, landmark \emph{events} are also being captured at scale by hundreds of cameras at major sports games, concerts, and political rallies. However, analogous large-scale reconstruction algorithms, that are able to fully leverage a large number of views of an event to produce long, dense, and accurate 3D trajectories, do not yet exist. %In this paper, we present a method for reasoning about the visibility of the target points at each camera view, to reconstruct the 3D motion of a dynamic scene from a large number of views.

Such video-based 3D motion reconstruction is challenging, as natural motion produces a greater occurrence of measurement loss due to occlusion and also causes artifacts in imagery (e.g., motion blur and texture deformation). Utilizing a large number of cameras can address these challenges, because it is likely to (1) narrow the average baseline between nearby cameras, (2) reduce the occurrence of occlusion, and (3) provide robustness to measurement noise due to the surplus views. However, previous approaches are unable to fully leverage the increasing number of views to improve 3D tracking performance (in terms of the average length of reconstructed trajectories, the density of the trajectories, and the accuracy of localization). The principal cause of failure emerges from errors in reasoning about the time-varying \emph{visibility} of dynamic 3D points. Poor visibility reasoning severely affects tracking performance, as an algorithm cannot benefit from an alternate viewpoint if it is unaware that the point is visible in the alternate view. Furthermore, an erroneous conclusion that a point is visible in a camera can bias the reconstruction, often producing a characteristic ``jump" artifact where a point assumes the identity of a different location.

%Existing approaches have addressed these challenges through structural regularization, either by assuming a fixed topology \cite{Furukawa2008, Beeler2011} or by spatially regularizing the trajectories \cite{Basha2012a,Huguet2007,Vogel2011}. However, the performance of such approaches are usually limited in terms of accuracy and density of the reconstructed motion trajectories, due to the and they are failed in producing practically applicable 3D motion due to the limitation of measurement and over-regularization available information. 
%have been proposed usually in a limited system setup with relatively small number of cameras, as shown in Table~\ref{Table:camSettup}. 

%Following static reconstruction algorithms, existing approaches reason about visibility (if at all) assuming photometric consistency among cameras where the target point is visible \cite{Carceroni2002,Devernay2006,Furukawa2008}: if the local appearance of a point matches the expected appearance, it is considered visible in that camera. However, photometric consistency tends to require frontal facing cameras (with respect to the tangent plane at the point), are susceptible to imaging artifacts, and require accurate estimates of the tangent plane normal at the point. Poor visibility reasoning severely affects tracking performance and obstructs the full benefit of a large number of unique viewpoints. 

In this paper, we demonstrate that precise inference of point visibility allows reconstruction algorithms to fully leverage large numbers of views to produce longer 3D trajectories with higher accuracy. In particular, our core algorithmic contributions are: (1) the use of motion consistency as a cue for the visibility of moving points; (2) the use of viewpoint regularity as a prior and a measure for viewpoint proximity; and (3) a maximum a posteriori (MAP) estimate for visibility estimation by probabilistically incorporating these cues with photometric and geometric consistency. We report empirical performance in reconstructing 3D motion captured by 480 cameras in scenes that contains significant occlusion, large displacement, and changes in the topology of the scene.


%The algorithm assumes that all cameras are intrinsically and extrinsically calibrated (e.g., using static reconstruction algorithms) and are temporally synchronized. We report empirical performance in reconstructing 3D motion captured by 480 VGA cameras using CMU Panoptic Studio~\cite{PanopticStudio2014}, in scenes that contains significant occlusion, large displacement, and changes in the topology of the scene.

%In this paper, we demonstrate that precise inference of point visibility allows video-based motion capture algorithms to fully leverage large number of views to produce longer 3D trajectories with greater accuracy. In particular, our core algorithmic contributions are: (1) the use of motion as a cue for the visibility of moving points; (2) the application of viewpoint regularity---that proximal viewpoints are likely to have similar visibility for a 3D point; and (3) the optimal estimation of visibility by dynamically fusing both these cues with appearance. The algorithm assumes that all cameras are intrinsically and extrinsically calibrated (e.g., using static reconstruction algorithms) and are temporally synchronized. We report empirical performance in reconstructing 3D motion captured by hundreds of cameras, in scenes that contains significant occlusion, large displacement, and changes in the topology of the scene.

%\color{red} ()Description of system and experiments )\color{black}

%\begin{table} [h]
%\centering
%\caption{Summary of number of cameras used in previous approaches}\label{Table:camSettup}
%\begin{tabular}{c|c}
%\hline 
%\scriptsize{Approaches} & \scriptsize{\# of cameras}\tabularnewline
%\hline 
%\hline 
%\scriptsize{Vedula et al.~\cite{Vedula2005}} & \scriptsize{17}\tabularnewline
%\hline 
%\scriptsize{Furukawa and Ponce~\cite{Furukawa2008} } & \scriptsize{22}\tabularnewline
%\hline 
%\scriptsize{Basha et al.~\cite{Basha2012a}} & \scriptsize{3}\tabularnewline
%\hline 
%\scriptsize{Huguet and Devernay~\cite{Huguet2007}} & \scriptsize{8}\tabularnewline
%\hline 
%\scriptsize{Carceroni and Kutalakos~\cite{Carceroni2002}} & \scriptsize{7}\tabularnewline
%\hline 
%\scriptsize{Varanasi and Zaharescu~\cite{Varanasi2008}} & \scriptsize{8}\tabularnewline
%\hline 
%\end{tabular} 
%\end{table}

\newpage

\section{Related Work}
%Dynamic 3D reconstruction approaches can be broadly categorized in methods that use dense multi-view reconstruction(e.g.,~\cite{de2008performance,de2007marker,starck2007surface}) and methods that use flow from multiple cameras (e.g.,~\cite{Vedula2005,Carceroni2002,Devernay2006}. The methods based on dense multi-view reconstruction typically use visual hull reconstruction to produce highly dense mesh model, but require subsequent processing to estimate 3D trajectories~\cite{de2008performance,starck2007surface}. Surface matching algorithms are used to provide dense correspondences between consecutive frames~\cite{Tung2010,Starck2005,Varanasi2008}. In these approaches, mesh models in each frame are independently generated using shape-from-silhouette techniques, and sparse matching between key mesh vertexes are performed using various cues such as 3D features and colors. Dense matching is then carried out based on estimating sparse matching using smoothing cost function using geodesic distance. The accuracy of motion estimation depends highly on the initially generated surfaces and their texture, and is limited by the vertex resolution. Silhouette-based methods also require stationary cameras to be able to estimate accurate silhouettes.
%
%%In comparison to silhouette-based reconstruction approaches, correspondence-based reconstruction methods produce sparser reconstructions, but do not require stationary cameras and can directly produce 3D trajectories. Among correspondence-based methods, perhaps the most related are scene flow reconstruction methods, introduced by Vedula et~al.~\cite{Vedula2005,Vedula1999}. Independently estimated 2D optical flow from multiple calibrated cameras was triangulated to generate the 3D flow, assuming that visibility was given a priori via reconstructed object shape. Several subsequent algorithms also have been proposed to recover both shape (depth) and motion simultaneously \cite{Basha2012a,Vogel2011,Huguet2007}. The basic assumption in these approaches is brightness constancy (or photometric consistency), which is used to determine the correspondences across views, and spatial regularization is used to condition the optimization and reduce the noise. While these approaches represent the target as a 3D points, other approaches use richer 3D representation such as dynamic surfels \cite{Carceroni2002,Devernay2006} or meshes \cite{Furukawa2008,Furukawa2009}. Mesh-based approaches have demonstrated robust results, producing trajectories of longer duration, but at the cost of assuming a fixed topology with a known mesh, and through the use of regularization.
%
%Typically, in previous work, only a small number of cameras are considered. In scene flow approaches, stereo cameras are usually used, and other approaches also use at most 10 to 20 cameras (17 by Vedula et al.~\cite{Vedula2005}, 22 by Furukawa and Ponce~\cite{Furukawa2008}, 8 by Huguet and Devernay~\cite{Huguet2007}, 7 by Carceroni and Kutalakos~\cite{Carceroni2002}). At this scale, information loss due to motion blur, texture deformation, occlusion, and self-occlusion are severe, and therefore necessitate significant spatiotemporal regularization of reconstructions. In most algorithms, precise camera visibility information is not considered, because the noise from a small number of outlier cameras can be ignored. Camera visibility is either assumed to be given by 3D reconstruction algorithm~\cite{Vedula2005} or handled by a robust estimator \cite{Vogel2011,Devernay2006,Basha2012a,Quiroga2012}. Patch-based methods use photometric consistency cue to determine visibility by comparing the texture across views \cite{Carceroni2002,Devernay2006,Furukawa2008}. However, these approaches require the texture of the 3D patch, and highly depends on the patch shape accuracy.
Dynamic 3D reconstruction approaches can be broadly categorized in methods that use silhouettes for reconstruction (e.g.,~\cite{de2008performance,de2007marker,starck2007surface,Zaharescu2011,Budd2013}) and methods that use correspondence for reconstruction (e.g.,~\cite{Vedula2005,Carceroni2002,Devernay2006}. Silhouette-based approaches typically use visual hulls to produce highly dense reconstruction, but require subsequent processing to estimate 3D trajectories~\cite{de2008performance,starck2007surface}. Surface matching algorithms are used to provide dense correspondences between consecutive frames~\cite{Tung2010,Starck2005,Varanasi2008}. In these approaches, mesh models in each frame are independently generated using shape-from-silhouette techniques, and sparse matching between key mesh vertexes are performed using various cues such as shape and appearance features. Dense matching is then carried out based on the sparse matches using a regularized cost function based on geodesic distance. The accuracy of motion estimation depends highly on the initial surface and texture, and is limited by the vertex resolution. Silhouette-based methods also require stationary cameras to be able to estimate accurate silhouettes.

In comparison to silhouette-based reconstruction approaches, correspondence-based methods produce sparser reconstructions, but do not require stationary cameras and can directly produce 3D trajectories. Among correspondence-based methods, perhaps the most related approaches are scene flow reconstruction methods, introduced by Vedula et~al.~\cite{Vedula2005}. Independently estimated 2D optical flow from multiple calibrated cameras was triangulated to generate the 3D flow, assuming that visibility was given a priori via reconstructed object shape. Several subsequent algorithms also have been proposed to recover both shape (depth) and motion simultaneously \cite{Basha2012a,Vogel2011,Huguet2007}. The basic assumption in these approaches is brightness constancy (or photometric consistency), which is used to determine the correspondences across views; spatial regularization is used to condition the optimization and reduce the noise. While these approaches represent the target as a 3D point, other approaches use richer 3D representation such as dynamic surfels \cite{Carceroni2002,Devernay2006} or meshes \cite{Furukawa2008}. Mesh-based approaches have demonstrated robust results, producing trajectories of longer duration, but at the cost of assuming a fixed topology with a known mesh, and through the use of regularization.

Typically, in previous work, only a small number of cameras are considered. In scene flow approaches, stereo cameras are usually used, and other approaches also use at most 10 to 20 cameras (17 by Vedula et al.~\cite{Vedula2005}, 22 by Furukawa and Ponce~\cite{Furukawa2008}, 8 by Huguet and Devernay~\cite{Huguet2007}, 7 by Carceroni and Kutalakos~\cite{Carceroni2002}). At this scale, information loss due to motion blur, texture deformation, occlusion, and self-occlusion are severe, and therefore necessitate significant spatiotemporal regularization of reconstructions. In most algorithms, precise camera visibility information is not considered, because the noise from a small number of outlier cameras can be ignored. Camera visibility is either assumed to be given by the 3D reconstruction algorithm~\cite{Vedula2005} or handled by a robust estimator \cite{Vogel2011,Devernay2006,Basha2012a,Quiroga2012}. Patch-based methods use photometric consistency to determine visibility by comparing the texture across views \cite{Carceroni2002,Devernay2006,Furukawa2008}. However, these approaches require the texture of the 3D patch, which depends heavily on the accuracy of the recovered patch shape.

%\color{red}
%%- In the related work section, the terminology silhouette-based method is inaccurate since any dense
%%multi-view reconstruction method can be used. It would be better to refer to these methods as dense
%%multi-view reconstruction methods. Also the following references are relevant and should be included:
%%Andrei Zaharescu, Edmond Boyer, Radu Horaud: Topology-Adaptive Mesh Deformation for Surface
%%Evolution, Morphing, and Multiview Reconstruction. IEEE Trans. Pattern Anal. Mach. Intell. 33(4): 823-
%%837 (2011)
%%Chris Budd, Peng Huang, Martin Klaudiny, Adrian Hilton: Global Non-rigid Alignment of Surface
%%Sequences. International Journal of Computer Vision 102(1-3): 256-270 (2013)
%%
%%\color{black}

% (To the best of our knowledge, motion cue is not yet carefully considered
% to determine camera visibility.)


%Markerless motion capture suffers from two main challenges: (1) establishing point correspondences across wide baseline cameras and (2) tracking 3D points that are consistent to the image motion. Two main approaches have been used to resolve the problems; representation and  



% Reconstructing a dynamic scene from multiple cameras is a core problem in 

% Reconstructing a dynamic scene from multiple cameras has been studied

% dynamic reconstruction...
%     scene flow\
%     multiview..
%     surface capture...
    
% fundamental problem is limited.
%     spatial regularity 
%     many cameras 
    
%     ==> visibiliity

% Many method in different area have have been proposed to reconstruct
% dynamic scene. The most related works with us are scene flow reconstruction
% methods, initially introduced by \cite{Vedula2000,Vedula2005}. In
% these works, scene flow is defined as a extension of optical flow
% to the 3D dimension, and it is estimated based on 2D optical flows
% from multiple views. Since calculating 2D optical flow is already
% ill posed problem, in the several follow-up algorithms, both 2.5 shape
% (or depth) and 3D flows are jointly estimated with spacial regularization
% such as variational framework\cite{Basha2012a,Huguet2007,Vogel2011}.
% Instead of using viewpoint-based representation, other approaches
% which utilize explicit 3D representations has been proposed to generate
% a complete view independent 3D scene descreption. In \cite{Carceroni2002,Devernay2006},
% surface patch model called \emph{surfel} is used, which enable to
% use texture information extracted from images. And, mesh representation
% is exploited also in \cite{Furukawa2008,Furukawa2009,Pons2007}, which
% produce much robust result than others with topological regularization.
% Instead of explicitly estimating motion, other approches mainly concentrate
% on the surface estimation of the dynamic scene\cite{de2008performance,de2007marker}. Even if these methods show an excellent results, their main focust is estimating accurate surface model, instead of accurate temporal motion. On the other hand, with the advancement of surface generation algorithms,
% surface matching algorithms have been also proposed to provide dense
% correspondences between consecutive frames~\cite{Tung2010,Starck2005,Varanasi2008,ahmed2008dense}. In these approaches, mesh models in each frame are independently generated
% from shape from silhouette techniques, and sparse matching between
% key mesh vertexes are performed using various cues such as protrude
% 3D feature and colors. Dense matching is carried out based on the
% estimating sparse matching using smoothing cost function using geodesic
% distance. Even if, the results shows visually successful result, their
% accuracy is highly depends on initially generated surfaces, and it
% is limited by the vertex resolution.

% However, only limited number of works carefully considered visibility issue. In some approaches, visibility is explicitly handled using know surface such as visual hull, including the initial scene flow approach~\cite{}. In the other scene flow approaches\cite{Basha2012a,Huguet2007,Vogel2011}, outlier camera view is handle by robust estimator function as proposed in \cite{Brox2004}. In particular, in patch based approaches, appearance feature is utilized by using estimated surface pose. In this approaches, a surface is visible from a view if the appearance is consistent with patches texture. In all of this approaches, visibility may not be the severe issue or can be ignored because the number of used camera is small, from stereo to up to 20 cameras. However, if the number of views are increasing, the reconstruction performance is severely affected by visibility, since number of outlier cameras also increases. 


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\columnwidth]{Notation_final}
\caption{The motion of a patch between time $t-1$ and $t$ is reconstructed from multiple cameras.}
\label{fig:notation}
\end{center}
\end{figure}
%\interfootnotelinepenalty=10000
%\vspace{-0.2cm}
\section{Notation}
%\vspace{-0.2cm}
Our algorithm takes, as input, image sequences from $N$ calibrated and synchronized cameras over $F$ frames and produces, as output, 3D trajectories of $P$ moving points with their instantaneous orientations and associated visibility in each camera frame. Since the method is applied to each point independently, we consider only a single point here to simplify the exposition.
 
As shown in Figure~\ref{fig:notation}, we track a parallelogram patch centered on a target 3D point $\mathbf{X} \in \mathds{R}^3$, whose extent is defined by two additional points $\mathbf{Y}^u$ and $\mathbf{Y}^v \in \mathds{R}^3$. The texture information $\mathbf{Q} \in \mathds{R}^m$ associated with the patch is defined by a unit vector concatenating normalized intensity values at a fixed number of grid positions on the patch, where $m$ is the number positions in the grid\footnote{The texture vector $\mathbf{\mathbf{Q}}$ is normalized as follows:
\begin{eqnarray}
\mathbf{Q} = \frac{1}{\sqrt{\sum_{j=1}^m (Q_j-\overbar{Q})^2}} \left[\begin{array}{c}Q_1-\overbar{Q}\\\vdots\\ Q_m-\overbar{Q} \end{array}\right], \label{Eq:normalized_texture}
\end{eqnarray}
where $\overbar{Q} = \sum_{j=1}^m Q_j / m$ and $Q_j$ is the $j^{\rm th}$ intensity value of the texture.
}. 
The patch $\mathbf{S}(t)$ is denoted by the set $\{\mathbf{X}(t), \mathbf{Y}^u(t), \mathbf{Y}^v(t),\mathbf{Q}(t)\}$, which is associated with the camera visibility set $\mathbf{V}(t) = \{\mathbf{v}_1(t),\cdots,\mathbf{v}_N(t)\}$, where $\mathbf{v}_i(t)$ is a binary value representing visibility with respect to  the $i^{\rm th}$ camera. % The normal $\mathbf{N}(t)$ is a unit vector orthogonal to the patch.
%\begin{equation}
%\mathbf{N}(t)= \frac{\left(\mathbf{Y}^u(t) - \mathbf{X}(t)\right) \times \left(\mathbf{Y}^v(t) - \mathbf{X}(t)\right)} {\left\| \left(\mathbf{Y}^u(t) - \mathbf{X}(t)\right) \times \left(\mathbf{Y}^v(t) - \mathbf{X}(t)\right) \right\|}. \nonumber
%\end{equation}
A 3D point is projected onto the $i^{\rm th}$ camera associated with a $3\times4$ projection matrix $\mathbf{P}_i$. The projection matrix is parametrized by a camera center vector $\mathbf{C}_i \in \mathds{R}^3$ and a 3$\times$3 rotation matrix $\mathbf{R}_i \in SO(3)$. The ``look-at" vector $\mathbf{o}_i$ is aligned with the $z$-axis of the camera, i.e., the third column of $\mathbf{R}_i^\mathsf{T}$. 

The 3D patch is projected onto the camera plane to form the projected patch $\mathbf{s}_i(t) = \{\mathbf{x}_i(t), \mathbf{y}_i^u(t), \mathbf{y}_i^v(t), \mathbf{q}_i(t) \}$, where $\mathbf{x}_i(t)$, $\mathbf{y}^u(t)$, and $\mathbf{y}^v(t) \in \mathds{R}^2$ are the projected points, i.e., $\widehat{\mathbf{x}}_i(t) \cong \mathbf{P}_i \widehat{\mathbf{X}}(t)$, $\widehat{\mathbf{y}}_i^u(t) \cong \mathbf{P}_i \widehat{\mathbf{Y}}^u(t)$, and $\widehat{\mathbf{y}}_i^v(t) \cong \mathbf{P}_i \widehat{\mathbf{Y}}^v(t)$, where $\widehat{\cdot}$ is the homogeneous coordinate representation of each vector. $\mathbf{q}_i  \in \mathds{R}^m$ is the texture information of the projected patch, which is defined by a concatenation of all the intensities from the $i^{\rm th}$camera, corresponding to the projected grid positions of $\mathbf{S}$, and normalized as in Equation~(\ref{Eq:normalized_texture}). Ideally, $\mathbf{Q}=\mathbf{q}_i$ if the 3D patch $\mathbf{S}$ is visible from the $i^{\rm th}$ camera, discounting illumination variation. We denote $\mathbf{m}_i$ as 2D optical flow at $\mathbf{x}_i(t-1)$ in the $i^{\rm th}$ camera, as shown in Figure~\ref{fig:notation}. 

The relationship between the $i^{\rm th}$ camera and patch can be defined by the co-visibility set $\mathbf{\Gamma}_i = \{\gamma^c_i, \gamma^p_i\}$, where
\begin{eqnarray}
\gamma^c_i = \frac{ (\mathbf{X}-\mathbf{C}_i)^\mathsf{T}\mathbf{o}_i} { || \mathbf{X}-\mathbf{C}_i || }~~~{\rm and}~~~\gamma^p_i = \frac{ (\mathbf{C}_i - \mathbf{X})^\mathsf{T}\mathbf{N}} { || \mathbf{C}_i - \mathbf{X} || }, \nonumber
\end{eqnarray}
$\gamma^c_i$ encodes the angle cosine of the patch location with respect to the camera ``look-at" vector $\mathbf{o}_i$ and $\gamma^p_i$ encodes the angle cosine of the camera location with respect to the 3D patch normal $\mathbf{N}$.

% In the tracking process, we compute 2D optical flow $\mathbf{m}_i$ at $\mathbf{x}_i(t-1)$ in the $i^{\rm th}$ camera as shown in Figure~\ref{fig:notation}. 


% We also define a scalar value $\gamma_i$ for the $i^{\rm th}$ camera representing the relationship between the camera center position and patch normal direction, as
% \begin{equation}
% \gamma_i = \frac{ (\mathbf{C}_i - \mathbf{X})^\mathsf{T}\mathbf{N}} { || \mathbf{C}_i - \mathbf{X} || }.
% \end{equation}
% A negative valued $\gamma_i$ means that the $i^{\rm th}$ camera center is located behind the patch.

\begin{figure}[t]
\includegraphics[width=1\columnwidth]{FlowChart}
\caption{Overview of patch tracking and visibility estimation.}
\label{fig:overview}
\end{figure}

%\vspace{-0.2cm}
\section{Overview}
%\vspace{-0.2cm}
At the initial time instance $t_0$, a target 3D patch is reconstructed and, over time, the algorithm alternately estimates the patch position and normal and its visibility with respect to all cameras. It should be noted that $t_0$ can be any arbitrary frame and that the tracking and the visibility computation are performed both forwards and backwards in time from $t_0$. We consider only forward tracking, from $t-1$ to $t$ to simplify the description. The flow chart of our algorithm is shown in Figure~\ref{fig:overview}.

\noindent \textbf{Patch Initialization.} Given the images from different cameras at the same time instance $t_0$, the algorithm reconstructs 3D points by matching features and triangulates them within a RANSAC framework. A 3D patch centered on $\mathbf{X}$ is reconstructed by maximizing the photometric consistency among the cameras where the patch is visible\footnote{The cameras that participate in RANSAC are used as an initial visible set, and the reference camera $\mathbf{P}_{\rm{ref}}$ is selected as the one closest to the initial 3D point in the iniler set. A 3D patch centered on $\mathbf{X}$ is initialized as a fixed scale square patch (40mm$\times$40mm), with $\mathbf{N}$ parallel to $\mathbf{o}_{\rm ref}$. We refine the patch based on the method described by Furukawa and Ponce~\cite{Furukawa2010} and select a new reference camera as the one closest to the current patch normal. The corresponding visibility set is updated by selecting cameras that have higher Normalized Cross Correlation (NCC) score than a threshold compared to $\mathbf{P}_{\rm{ref}}$. Within the patch initialization process, the normal refinement and visibility update are iterated.}. This initializes $\mathbf{S}(t_0)$ and $\mathbf{V}(t_0)$.

\noindent \textbf{Patch Tracking.} Given the previously obtained 3D patch $\mathbf{S}(t-1)$ and visibility $\mathbf{V}(t-1)$, the algorithm estimates the next 3D patch $\mathbf{S}(t)$ based on 2D optical flow in the cameras defined by $\mathbf{V}(t-1)$. For the $i^{\rm th}$ camera in $\mathbf{V}(t-1)$, optical flow~\cite{Lucas1981} is estimated at multiple scales at the points $\mathbf{x}_i(t-1)$, $\mathbf{y}^u_i(t-1)$, and $\mathbf{y}^v_i(t-1)$. To eliminate unreliable flow, a backward-forward consistency check~\cite{sundaram2010dense} is performed for flow at each scale and only the most reliable flow is retained. The next 3D positions, $\mathbf{X}(t)$, $\mathbf{Y}^{u}(t)$, and $\mathbf{Y}^{v}(t)$, are estimated by triangulating optical flow outputs within a RANSAC framework. The RANSAC process is crucial since $\mathbf{V}(t-1)$ may not be valid anymore at time $t$, due to motion. After RANSAC, the normal is refined by maximizing the photometric consistency among the images that belong to the inliers of RANSAC, as in the patch initialization process.

% ignored footnote
%\footnote{In practice, optical flow is computed from all the images regardless membership of visibility, since, in the end, it is used to compute the visibility likelihood based on motion-based consistency for all the cameras, as described in \ref{sub:Motion-Cue}}

\noindent \textbf{Visibility Estimation.} Based on the reconstructed $\mathbf{S}(t)$ and its motion from $\mathbf{S}(t-1)$, our approach finds the MAP estimate of the current visibility set $\mathbf{V}(t)$ by fusing photometric consistency, motion consistency, and geometric consistency, in conjunction with a Markov Random Field (MRF) prior. Typically, the tracking process is severely affected by false positive cameras where the target is not visible. Poor visibility reasoning at the RANSAC stage can cause a characteristic ``jump" error to a different scene point, and also reduces the normal refinement performance causing frequent local minima during the optimization process. Our precise visibility estimation results in longer trajectories of higher accuracy.

Patch tracking and visibility estimation are interdependent processes. At each time instance, we can iterate these two procedures until convergence; in practice, a single iteration is usually sufficient.  

\section{Visibility Estimation\label{sec:Optimal-Visibility-Estimation}}
%In this section, we model visibility estimation as MAP inference using various cues presented in the subsequent subsections. In subsection~\ref{sub:Photo-Cue}, we describe appearance consistency, a common way to deal with visibility~\cite{Carceroni2002,Devernay2006,Furukawa2008}. Then, we present a novel cue, motion consistency for visibility estimation (subsection~\ref{sub:Motion-Cue}). We also model additional cues the relation between camera centers and patch normal direction (subsection~\ref{sub:Normal-cue}), and the prior likelihood of two cameras to be in a visible set using a Markov Random Field (subsection~\ref{sub:prior}). Finally, we describe an algorithm to optimally estimate visibility based on all the available cues by finding the minimum cut of a capacitated graph over cameras (subsection~\ref{sub:graphcuts}).

In this section, we present a method to compute the maximum a posteriori (MAP) estimate of visibility $\mathbf{V}$ using photometric consistency, motion consistency, and geometric consistency, with a proximity prior. These cues are represented using 2D texture $\{ \mathbf{q}_i \}_{i=1}^N$, 2D optical flow $\{ \mathbf{m}_i \}_{i=1}^N$, and the co-visibility set $\{ \mathbf{\Gamma}_i \}_{i=1}^N$. Given these cues and by applying Bayes theorem, the probability of visibility is
\begin{align}
&P( \mathbf{V} | \mathbf{q}_1, \mathbf{m}_1, \mathbf{\Gamma}_1, \cdots, \mathbf{q}_N,  \mathbf{m}_N, \mathbf{\Gamma}_N ) \nonumber\\
&\propto P(\mathbf{q}_1, \mathbf{m}_1, \mathbf{\Gamma}_1, \cdots, \mathbf{q}_N,  \mathbf{m}_N, \mathbf{\Gamma}_N |  \mathbf{V}) P(\mathbf{V}).  \nonumber
\end{align}
%\begin{eqnarray}
%&& P( \mathbf{V} | \mathbf{q}_1, \mathbf{m}_1, \mathbf{\Gamma}_1, \cdots, \mathbf{q}_N,  \mathbf{m}_N, \mathbf{\Gamma}_N ) \nonumber\\
%&\propto& P(\mathbf{q}_1, \mathbf{m}_1, \mathbf{\Gamma}_1, \cdots, \mathbf{q}_N,  \mathbf{m}_N, \mathbf{\Gamma}_N |  \mathbf{V}) P(\mathbf{V}).  \nonumber
%\end{eqnarray}

%Assuming that (1) the probability of cues for each camera is conditionally independent given visibility and (2) that cue is conditionally independent given the visibility of its camera, the probability can be written as
Given the visibility of each camera, we assume that (1) the cues in that camera are conditionally independent to the cues in other cameras and the visibility of other cameras, (2) that each cue within the same camera is conditionally independent to each other. The probability can be written as
\begin{eqnarray}
%&=& \left( \prod_{i=1}^{N} P( \mathbf{q}_i,\mathbf{m}_i,\gamma_i | \mathbf{v}_1, \cdots, \mathbf{v}_N) \right) P(\mathbf{v}_1, \cdots, \mathbf{v}_N) \nonumber\\
%&=& \left( \prod_{i=1}^{N} P( \mathbf{q}_i,\mathbf{m}_i,\gamma_i | \mathbf{v}_i) \right) P(\mathbf{v}_1, \cdots, \mathbf{v}_N) \nonumber\\
\left( \prod_{i=1}^{N} P( \mathbf{q}_i | \mathbf{v}_i )P( \mathbf{m}_i | \mathbf{v}_i) P( \mathbf{\Gamma}_i| \mathbf{v}_i) \right)P(\mathbf{V}).
\label{eq:initProb}
\end{eqnarray}
The MAP estimate of visibility $\mathbf{V}^*$ can be obtained by maximizing the expression in Equation~(\ref{eq:initProb}), i.e., 
\begin{eqnarray}
\mathbf{V^*} = \underset{\mathbf{V}}{\operatorname{argmax}}  \left( \prod_{i=1}^{N} P( \mathbf{q}_i | \mathbf{v}_i )P( \mathbf{m}_i | \mathbf{v}_i)P( \mathbf{\Gamma}_i| \mathbf{v}_i) \right) P(\mathbf{V}), \nonumber
\end{eqnarray}
or equivalently,
\begin{eqnarray}
\mathbf{V^*} =\underset{\mathbf{V}}{\operatorname{argmax}}  \sum_{i=1}^{N} \log P( \mathbf{q}_i | \mathbf{v}_i)  + \sum_{i=1}^{N} \log P( \mathbf{m}_i | \mathbf{v}_i ) +\nonumber\\
   + \sum_{i=1}^{N} \log P( \mathbf{\Gamma}_i| \mathbf{v}_i) + \log P(\mathbf{V}).
\label{eq:goalFunction}
\end{eqnarray}
We describe the probability of each cue and the prior in the subsequent sub-sections, and compute the MAP estimate by finding the minimum cut of a capacitated graph over cameras~\cite{boykov2001fast}. 
 
%model visibility estimation as MAP inference using various cues presented in the subsequent subsections. Our goal is finding optimal visibility given all the available cues --- the 2D texture $\{ \mathbf{q}_i \}_{i=1}^N$, 2D flow $\{ \mathbf{m}_i \}_{i=1}^N$, and camera center positions $\{ \mathbf{C}_i \}_{i=1}^N$ with respect to the patch normal $\mathbf{N}$. Given $\{\mathbf{q}_i, \mathbf{m}_i, \mathbf{C}_i\}_{i=1}^N $, we find the visibility set $\mathbf{V} \in \{0,1\}^{\rm N}$ which maximizes the following probability:

%where we assume that each cue in each camera is conditionally independent given visibility $\mathbf{v}_i$. The optimal visibility can then be estimated by maximizing Equation~\ref{eq:initProb}


%Using the probability defined above, 
%
%\begin{eqnarray}
%P(m_i~|~v_i) &=& exp(\frac{\|x_i-m_i\|^2}{\sigma^2}) \nonumber\\
%P(p_i~|~v_i) &=& exp(\kappa \mu^\mathsf{T}p_i) \nonumber\\
%P(n_i~|~v_i) &=& T(o_t^{\mathsf{T}}n_i < \theta) / \tau
%\end{eqnarray}
%

\subsection{Photometric consistency}\label{sub:Photo-Cue}
%\subsection{Photometric Consistency for Visibility Likelihood}\label{sub:Photo-Cue}
Photometric consistency has been widely used for reasoning about visibility~\cite{Snavely:2006,Frahm:2010,Furukawa:2010,Furukawa2008,Devernay2006}. It measures the correlation between the texture $\mathbf{Q}$ of a 3D patch and the texture $\mathbf{q}_i$ of the corresponding patch in the $i^{\rm th}$ camera. Normalized Cross Correlation (NCC) is one such measure of photometric consistency, which is robust to illumination variation. Since $\mathbf{Q}$ and $\mathbf{q}_i$ are defined as normalized unit vectors by Equation~(\ref{Eq:normalized_texture}), $\mathbf{Q}^\mathsf{T}\mathbf{q}_i$ measures the NCC. We model the probability distribution of $\mathbf{q}_i$ using a von Mises-Fisher distribution around $\mathbf{Q}$, i.e., $\mathbf{q}_i\sim \mathcal{V}(\mathbf{Q}, \kappa)$, which is defined by $\mathbf{Q}^\mathsf{T}\mathbf{q}_i$. $\kappa$ is a concentration parameter that controls the degree of variation of the texture. Lower values of $\kappa$ allows more variation between $\mathbf{Q}$ and $\mathbf{q}_i$. From the distribution, we can describe the logarithm of the probability of $\mathbf{q}_i$ given $\mathbf{v}_i$ as 
\begin{eqnarray}
%P(\mathbf{q}_i | \mathbf{v}_i) = C_p\exp(\kappa\mathbf{Q}^\mathsf{T}\mathbf{q}_i).
\log P(\mathbf{q}_i | \mathbf{v}_i) \propto \kappa\mathbf{Q}^\mathsf{T}\mathbf{q}_i.
\label{eq:eq_appearance}
\end{eqnarray}

\subsection{Motion Consistency}\label{sub:Motion-Cue}
In dynamic scenes, motion is an informative cue for determining visibility. Given the 3D motion of a patch, the observed optical flow at the $i^{\rm th}$ camera must be consistent with the projected 3D motion of the target patch, if the patch is visible from the camera view. In other words, motion consistency requires that 2D optical flow $\mathbf{m}_i$ must be consistent with the projected displacement of the 3D motion $\mathbf{x}_i(t) - \mathbf{x}_i(t-1)$.

%, where 
%\begin{eqnarray}
%\mathbf{P}_i \mathbf{X}(t) = \mathbf{P}_i \mathbf{X}(t-1) + \mathbf{m}_i ,
%\mathbf{x}_i(t) = \mathbf{x}_i(t-1) + \mathbf{m}_i,
%\end{eqnarray} 
We model the probability distribution of $\mathbf{m}_i$ using a normal distribution around the projected 3D displacement, i.e., $\mathbf{m}_i \sim \mathcal{N}\left( \mathbf{x}_i(t) - \mathbf{x}_i(t-1), \sigma \right) $, where $\sigma$ is the standard deviation capturing the certainty of the 3D motion estimation in pixel units. Therefore, the log likelihood can be written as 
\begin{eqnarray}
\log P( \mathbf{m}_i | \mathbf{v}_i) \propto -\frac{\| \mathbf{m}_i - \left( \mathbf{x}_i(t) - \mathbf{x}_i(t-1) \right)  \|^2}{2\sigma^2}.
\label{eq:eq_motion}
\end{eqnarray}

Motion consistency is a necessary condition. We now characterize cases when the motion consistency cue is ambiguous. Let $\mathbf{X}(t)$ and $\mathbf{X}'(t)$ be two distinct points in 3D space. Motion consistency cue is ambiguous if and only if the following two conditions hold:
\begin{eqnarray}
\mathbf{P}_i {\widehat{\mathbf{X}}}(t)&\cong&\mathbf{P}_i {\widehat{\mathbf{X}}'}(t)\label{Eq:projection1}  \nonumber \\
\mathbf{P}_i {\widehat{\mathbf{X}}}(t+1)&\cong&\mathbf{P}_i {\widehat{\mathbf{X}}'}(t+1)\label{Eq:projection2},
\label{eq:ambiguousMotion}
\end{eqnarray}
where $\| \mathbf{X} - \mathbf{C}_i \| > \| \mathbf{X'} - \mathbf{C}_i \|$, i.e., $\mathbf{X}'(t)$ occludes $\mathbf{X}(t)$ for the $i^{\rm th}$ camera.
% , and assume that $\mathbf{X}'(t)$ is occluding $\mathbf{X}(t)$ in the $i^{\rm th}$ camera's view as
% \begin{eqnarray}
% \mathbf{P}_i {\widehat{\mathbf{X}}}(t)&=&\mathbf{P}_i {\widehat{\mathbf{X}}'}(t)\\
% \mathbf{P}_i {\widehat{\mathbf{X}}}(t+1)=\mathbf{P}_i {\widehat{\mathbf{X}}'}(t+1)
% \end{eqnarray}
% where $\| \mathbf{X} - \mathbf{C}_i \| > \| \mathbf{X'} - \mathbf{C}_i \|$. Motion consistency is ambiguous if,
% \begin{eqnarray}
% \mathbf{P}_i {\widehat{\mathbf{X}}}(t+1)=\mathbf{P}_i {\widehat{\mathbf{X}}'}(t+1).
% \label{eq:ambiguousMotion}
% \end{eqnarray}
In a static scene, motion does not exist and thus, the motion consistency cue is always ambiguous because $\mathbf{X}(t) = \mathbf{X}(t+1)$ and $\mathbf{X'}(t) = \mathbf{X'}(t+1)$. Another case that occurs in practice is when the occluding patch and the occluded patch lie on a body undergoing global translational motion, under a camera that approaches orthographic projection. 

%Let the points undergo affine transform between frames as follows:
We characterize the set of ambiguous motions where Equation (\ref{eq:ambiguousMotion}) holds, assuming that $\mathbf{X}(t)$ and $\mathbf{X}'(t)$ undergo the same affine transform between frames, as 
\begin{eqnarray}
\mathbf{X}(t+1)&=&\mathbf{A} \mathbf{X}(t) +  \mathbf{a} \label{Eq:transform1}  \nonumber \\
\mathbf{X'}(t+1)&=&\mathbf{A} \mathbf{X'}(t) + \mathbf{a} \label{Eq:transform2}, 
\end{eqnarray}
where $\mathbf{A} \in \mathds{R}^{3 \times 3}$ and  $\mathbf{a} \in \mathds{R}^{3}$ represent a 3D affine transform. The motion consistency cue is ambiguous if and only if the following condition holds:
\begin{eqnarray}
\mathbf{X} \in {\rm null}\left( [\mathbf{a}]_{\times} \mathbf{A} \right), \label{Eq:nullspace}  
\end{eqnarray}
where ${\rm null}(\cdot)$ is the null space of $\cdot$. See the Appendix for a proof. In ideal cases with infinite precision and zero measurement noise, this condition rarely occurs (if there is motion).

%
%
%Under the action of $\mathbf{A}$, 
%\begin{eqnarray}
%{\mathbf{X}}(t+1)=\mathbf{A} {\mathbf{X}}(t), \label{eq:trans}\\
%{\mathbf{X}}'(t+1)=\mathbf{A} {\mathbf{X}}'(t).
%\end{eqnarray}
%
%
%
%From these equations, we can have the following linear systems,
%\begin{eqnarray}
%\mathbf{0}&=&\left[\begin{array}{c} \mathbf{P} \\ \mathbf{P} \mathbf{A}\end{array}\right] (\mathbf{X}(t)-{\mathbf{X}(t+1)})\nonumber\\
%&=&\left[\begin{array}{ccc} 1 & 0 & 0\\ 0 & 1 & 0 \\ a_{11}& a_{12}&a_{13}\\a_{21}& a_{22}&a_{23}\end{array}\right] (\mathbf{X}(t)-{\mathbf{X}(t+1)})\nonumber\\
%&=& \mathbf{D} (\mathbf{X}(t)-{\mathbf{X}(t+1)}).
%\end{eqnarray}
%where $a_{ij}$ is the $i,j$ element in $\mathbf{A}$. If a null space of $\mathbf{D}$ exists, the cue is ambiguous because there exists infinitely number of distinct ${\mathbf{X}(t+1)}$s. The first two rows in $\mathbf{D}$ are linearly independent and $\mathbf{D}$ can be rank-deficient only when the last two rows are spanned by the first two rows. This results in $a_{13}=0$ and $a_{23}=0$. Thus, it can be seen that if the object undergoes a linear transform $\mathbf{A}$, motion becomes ambiguous, if $\mathbf{A}$ has the following form,
%\begin{eqnarray}
%\mathbf{A} = \left[\begin{array}{ccc} \times & \times & 0 \\
%                                       \times & \times & 0 \\
%                                   \times & \times & \times\end{array} \right]. \nonumber
%\end{eqnarray}
%\color{black}


\begin{figure}[t]
\includegraphics[width=1\columnwidth]{NormalConstraint}
\caption{(a) The valid region filtered by  $\gamma_p$ and $\tau_p$ is shown as a shaded region. The angle limitation with respect to the $\mathbf{N}$ is computed as $\cos^{-1}{\tau_p}$. (b) 
$g_\textrm{s}$ computed by two cameras are shown as a shaded polygon, where $\int_V \mathcal{H} (\mathbf{o}_i^{\mathsf{T}} \mathbf{o}_j) F_i(v) F_j(v) {\rm d}v >0$. (c) An example where $\mathcal{H} (\mathbf{o}_i^{\mathsf{T}} \mathbf{o}_j)=0$ is shown. An oriented patch cannot be visible by two cameras facing each other simultaneously.}
\label{fig:PatchRelationMRF}
\end{figure}

\subsection{Geometric consistency} \label{sub:Normal-cue}

Oriented patches are only visible from cameras whose ``look-at" vector $\mathbf{o}_i$ is in the opposite direction to the patch normal $\mathbf{N}$ and in front of it. We incorporate this geometric cue based on the co-visibility set $\mathbf{\Gamma}_i$ considering the camera position relative to the patch normal direction and the patch position relative to the camera ``look-at" vector. The probability of $\mathbf{\Gamma}_i$, given visibility $\mathbf{v}_i$, can be written as 
%(We will show a couple of exceptional cased in our experimental results), i.e., camera view angle with respect to the current patch normal:
\begin{eqnarray}
P(\mathbf{\Gamma}_i | \mathbf{v}_i ) = \begin{cases} \frac{1}{ (1- \tau_c)(1- \tau_p)} & {\rm if~} \gamma_i^c \geq \tau_c,{\rm and~} \gamma_i^p \geq  \tau_p  \\
                                                           0 & {\rm otherwise,}\end{cases} 
                                                           %P(\mathbf{C}_i ~|~ \mathbf{v}_i = 1) &=& \begin{cases} \frac{1}{\rho} & {\rm if~} \mathbf{N}^\mathsf{T} \overline{\mathbf{X} \mathbf{C}_i}  > \tau\\\
%                                                           0 & {\rm otherwise.}\end{cases}
%P(\mathbf{c}_i ~|~ \mathbf{v}_i = 1) &=& T(o_t^{\mathsf{T}}n_i < \theta) / \tau
\label{eq:eq_normal}
\end{eqnarray}
where $\tau_c<1 $ is the cosine angle representing the field of view of the camera, and $\tau_p<1$ is a threshold (cosine angle) to determine the angular visibility with respect to the patch normal. Figure~\ref{fig:PatchRelationMRF}(a) shows an example of the cue, where the shaded area represents the valid region according to $\tau_p$.

%where $\overline{\mathbf{X} \mathbf{C}_i}$ is a unit vector from target point (patch center) to the camera center, $\rho$ is a normalization factor so that sum of the distribution becomes one. $\tau$ is threshold to determine a angle limitation where the patch is visible. 

\subsection{Visibility Regularization Prior} \label{sub:prior}
Under a Markov Random Field prior over camera visibility, we decompose the joint probability of visibility $P(\mathbf{V})$ into pairwise probabilities, i.e.,
\begin{eqnarray}
P(\mathbf{v}_1, \cdots, \mathbf{v}_N)= \prod_{i,j \in \mathcal{G}(i)} P(\mathbf{v}_i, \mathbf{v}_j),
\end{eqnarray}
where $\mathcal{G}(i)$ is the set of adjacent camera indices of the $i^{\rm th}$ camera. This decomposition captures the prior distribution of visibility, representing the prior that two cameras that have similar viewpoints are likely to have consistent visibility. This proximity constraint constitutes prior knowledge that can regularize noisy visibility when both photometric consistency and motion consistency cues are weak (e.g., due to motion blur in an individual camera). We model the log likelihood of the joint probability as follows:
\begin{eqnarray}
\log P(\mathbf{v}_1, \cdots, \mathbf{v}_N) \propto \sum_{i,j \in \mathcal{G}(i)} g_s (\mathbf{v}_i,\mathbf{v}_j),
\label{eq:eq_mrf}
\end{eqnarray}
where $g_s$ is defined by the cost between two cameras using the overlapping volume of the two camera frustums. This is estimated as follows:
\begin{eqnarray}
g_\textrm{s}(\mathbf{P}_i,\mathbf{P}_j) = \frac{\int_V \mathcal{H} (\mathbf{o}_i^{\mathsf{T}} \mathbf{o}_j) F_i(v) F_j(v) {\rm d}v} {\int_V F_i(v) + F_j(v) - F_i(v)F_j(v) {\rm d}v}, 
%g_\textrm{s}(\mathbf{P}_i,\mathbf{P}_j) = \frac{|\int_V \mathcal{H} (\mathbf{o}_i^{\mathsf{T}} \mathbf{o}_j) F_i(v) F_j(v) {\rm d}v|^2}{\int_V F_i(v) {\rm d}v \int_V F_j(v) {\rm d}v},
\end{eqnarray}
where $v$ is an infinitesimal volume in the working space $V$ (see Figure~\ref{fig:PatchRelationMRF}(c)). $F_i(v)$ is a binary function defined as 
\begin{eqnarray}
F_i(v) = \begin{cases} 1 & {\rm if~}v~{\rm is~visible~from~the~}i^{\rm th} {\rm~camera}
					\\ 0 & {\rm otherwise.}
		\end{cases}
\end{eqnarray}
$\mathcal{H}$ is a Heaviside step function to take into account a pair of cameras oriented in similar directions. Equation~(\ref{eq:eq_mrf}) captures the ratio between the volume of the intersections of camera frustums and the volume of the union of camera frustums. Figure~\ref{fig:PatchRelationMRF}(b) illustrates $g_\textrm{s}$ where the shaded polygon represents $\int_V \mathcal{H} (\mathbf{o}_i^{\mathsf{T}} \mathbf{o}_j) F_i(v) F_j(v) {\rm d}v$, and Figure~\ref{fig:PatchRelationMRF}(c) shows an example where $\mathcal{H} (\mathbf{o}_i^{\mathsf{T}} \mathbf{o}_j)=0$.

%i.e.,
%\begin{eqnarray}
%\mathccal{H}(o_i,o_j) = \begin{cases} 1 & {\rm if~} o_i^\matthsf{T}o_j >0
%					\\ 0 & {\rm otherwise.}
%		\end{cases}
%\end{eqnarray}

In practice, we discretize the working volume using voxels and count the number of common voxels that are projected inside both cameras. This enables us to reward consistent visibilities in proximal cameras. %Figure~\ref{fig:MRF}(b) and (c) shows examples of camera relation and computed edge cost. %, and Figure~\ref{fig:MRF}(b),(c) show the generated camera graph in our system setup with calculated edge cost. 

\subsection{MAP Visibility Estimation via Graph Cuts\label{sub:graphcuts}} 
We incorporate Equations~(\ref{eq:eq_appearance}), (\ref{eq:eq_motion}), (\ref{eq:eq_normal}), and (\ref{eq:eq_mrf}) into Equation~(\ref{eq:goalFunction}) to find the MAP estimate of visibility $\mathbf{V^*}$ and, therefore, Equation~(\ref{eq:goalFunction}) can be rewritten as:
\begin{eqnarray}
\mathbf{V^*} = \underset{\mathbf{V}}{\operatorname{argmin}}  \sum_{i=1}^{N} E_d( \mathbf{v}_i) + \sum_{ i, j \in \mathcal{G}(i) } E_s (\mathbf{v}_i,\mathbf{v}_j),
\end{eqnarray}
where $E_d$ encodes photometric consistency, motion consistency, and geometric consistency, and $E_s$ encodes the prior between cameras. 
%\begin{eqnarray}
%E_d(\mathbf{v}_i) =& \frac{\| \mathbf{m}_i - ( \mathbf{x}_i(t) - \mathbf{x}_i(t-1) )  \|^2} {2\sigma^2} -\kappa {\mathbf{Q}_i}^\mathsf{T} \mathbf{q}_i + \delta(\gamma_i ) \nonumber\\
%E_s(\mathbf{v}_i,\mathbf{v}_j) =& \begin{cases} -g_\textrm{s}(\mathbf{P}_i,\mathbf{P}_j) & {\rm if~} \mathbf{v}_i=\mathbf{v}_j  \nonumber\\                                                             
%0 & {\rm otherwise,}\end{cases} 
%\end{eqnarray}
\begin{eqnarray}
E_d(\mathbf{v}_i) =& \frac{\| \mathbf{m}_i - ( \mathbf{x}_i(t) - \mathbf{x}_i(t-1) )  \|^2} {2\sigma^2} -\kappa {\mathbf{Q}_i}^\mathsf{T} \mathbf{q}_i + \delta(\mathbf{\Gamma}_i ) \nonumber\\
E_s(\mathbf{v}_i,\mathbf{v}_j) =& \begin{cases} 0 & {\rm if~} \mathbf{v}_i=\mathbf{v}_j  \nonumber\\                                                             
		g_\textrm{s}(\mathbf{P}_i,\mathbf{P}_j) & {\rm otherwise,}\end{cases} 
\end{eqnarray}
where $\delta = \log(1-\tau_c)(1-\tau_p)$  if $\gamma_i^c > \tau_c$ and $\gamma_i^p>\tau_p$, or $\delta=\infty$, otherwise. This minimization problem can be optimally computed via graph cuts~\cite{boykov2001fast}. 
%originates from the distribution related to patch's normal direction (Equation~\ref{eq:distributionNormal}). $\delta$ is infinity value when $P(\mathbf{C}_i ~|~ \mathbf{v}_i = 1) = 0$, or zero otherwise. 

\begin{figure*}[ht]
  \centering       
%  \subfigure[An input image]{\label{Fig:visEx_target}\includegraphics[height=0.15\textwidth]{Target}}
    \subfigure[Selected patch]{\label{Fig:visEx_Input}\includegraphics[height=0.18\textwidth]{GT_input_final}}
  \subfigure[Ground truth]{\label{Fig:visEx_GT}\includegraphics[height=0.18\textwidth]{GT}}
   \subfigure[Photo-consistency]{\label{Fig:visEx_App}\includegraphics[height=0.18\textwidth]{AppOnly}} 
    \subfigure[MAP] {\label{Fig:visEx_MAP}\includegraphics[height=0.18\textwidth]{MAP}} 
    \subfigure[Visibility Accuracy Comparison]{\label{Fig:VisibilityError}\includegraphics[height=0.18\textwidth]{VisibilityError2}}
%    \subfigure[Failure case of MAP estimate]{\label{Fig:MAP_failure}\includegraphics[height=0.22\textwidth]{MAP_failure_example}} 
 	\caption{The red arrow denotes the normal vector of the selected patch. The pyramid structures represent camera poses, where blue cameras belong to the visible set (we warp the camera positions for better visualization). (a) The selected patch is shown in 3D view and 2D image. (b) We manually generate ground truth visibility. (c) Visibility estimated by the baseline. (d) Visibility estimated by our method. (e) We compare accuracy of visibility estimates of both methods.} 
  \label{Fig:visibilityError}
\end{figure*}

\section{Results}
% \begin{figure}
% \includegraphics[width=1\columnwidth]{images/CameraGraph}

% \caption{Calibrated 480 cameras and their graph }


% \label{fig:CameraGraph}
% \end{figure}


% \subsection{System Setup and Datasets}

We evaluate our algorithm on a variety of challenging scenes in the presence of significant occlusion (Circular Movement and Falling Boxes), large displacement (Confetti and Fluid motion), and topological change (Falling boxes and Volleyball). Our visibility estimation enables us to better leverage a large number of cameras in producing accurate and long trajectories. The dataset used in the evaluation is summarized in Table~\ref{Table:dataset} and is available on the project website. The sequences were captured at the CMU Panoptic Studio~\cite{PanopticStudio2014} containing 480 cameras capturing 640$\times$480 video at 25 Hz. The cameras are extrinsically and intrinsically calibrated, and are synchronized via an external clock.

{
\begin{table} [h]

\centering
\caption{Summary of the datasets.}\label{Table:dataset}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\hline 
\scriptsize{Sequence} & \scriptsize{Frames} & \scriptsize{Duration} & \scriptsize{\# of points}  & \scriptsize{Av. traj. length}\tabularnewline
\hline 
\hline 
\scriptsize{Circ. Movement} & \scriptsize{250} & \scriptsize{10.0 sec} & \scriptsize{10433} & \scriptsize{404.9 cm}  \tabularnewline
\hline 
\scriptsize{Volleyball} & \scriptsize{210} & \scriptsize{8.4 sec} & \scriptsize{8422} & \scriptsize{326.4 cm} \tabularnewline
\hline 
\scriptsize{Bat Swing} & \scriptsize{200} & \scriptsize{8.0 sec} & \scriptsize{3849} & \scriptsize{224.1 cm} \tabularnewline
\hline 
\scriptsize{Falling Boxes} & \scriptsize{160} & \scriptsize{6.4 sec} & \scriptsize{17934} & \scriptsize{164.7 cm}\tabularnewline
\hline 
\scriptsize{Confetti} & \scriptsize{200} & \scriptsize{8.0 sec} & \scriptsize{10345} & \scriptsize{103.0 cm} \tabularnewline
\hline 
\scriptsize{Fluid Motion} & \scriptsize{200} & \scriptsize{8.0 sec} & \scriptsize{3153} & \scriptsize{123.1 cm} \tabularnewline
\hline 
\end{tabular} 
}
\end{table}
}
% foot note for \label{Fig:visibilityError}
%\footnotetext{We warp the camera positions for better visualization.}

\subsection{Quantitative Evaluation}
\noindent\textbf{Visibility Estimation Accuracy.} We select an arbitrary patch in the Circular Movement sequence reconstructed at a time instance, and manually generate ground-truth visibility data at each sampled time instance by selecting cameras where the target patch is visible. We compare our visibility estimation method (MAP) against a baseline method based on photometric consistency alone, which is a cue commonly used by previous approaches~\cite{Carceroni2002,Devernay2006,Furukawa2008}. Visibility estimation results generated from each method at a time instance are visualized in Figure~\ref{Fig:visibilityError}. As a criterion, we compute the true positive detection rate between the ground truth data and $\mathbf{V}(t)$ estimated by both methods. The true positive rate from each method is shown in Figure~\ref{Fig:VisibilityError}, demonstrating that our method outperforms the baseline method by a significant margin.\\ %In particular, almost all of error of the proposed method are caused by the cameras located at extreme view angle with respect to the patch where it is hard to determine the ground truth value. 
\noindent \textbf{Tracking Accuracy and Length.}
We evaluate our method considering both tracking accuracy and trajectory length. Inspired by the evaluation criterion proposed by Furukawa and Ponce~\cite{Furukawa2008}, a test sequence is generated by appending it at the end of itself in reverse order, and the tracking algorithm is performed on the generated sequence. The tracked patches must return back to the original position, if tracking is accurate. In this experiment, the 3D error is defined by the 3D distance between initial and the final locations of the target point. We generate five test sequences using the Circular Movement sequence by changing the duration (10 to 50 frames) from a fixed initial frame. For the evaluation, we count the number of successfully reconstructed trajectories that have less than 2 cm drift error. Figure~\ref{Fig:traj_distance_histo} shows a histogram of the number of trajectories using 480 cameras.  Our MAP estimate method outperforms the method based on photometric consistency in terms of both number of trajectories and length of trajectories. We also perform experiments with different number of cameras by uniformly sampling cameras to examine its impact on tracking success rate. Figure~\ref{Fig:survivedPt} shows how our method leverages a large number of cameras. Note that the number of successfully tracked trajectories increases faster than the method based on photometric consistency. 

\begin{figure}[t]
  \centering       
    \subfigure[Tracking Performance]{\label{Fig:traj_distance_histo}\includegraphics[width=0.22\textwidth]{Circular_Cam480_pt_length3}}
    \subfigure[Number of cameras]{\label{Fig:survivedPt}\includegraphics[width=0.22\textwidth]{Circular_PtNum_CamNum3}}     
 \caption{(a) Our MAP estimate outperforms the baseline method in terms of the number of trajectories and the length of trajectories. (b) Our method leverages the large number of views, and shows a faster increasing curve than the baseline method.} 
  \label{Fig:quantitative}
\end{figure}

\begin{figure*}[th]
  \centering       
  \includegraphics[width=0.8\textwidth]{VisibilityQualitativeFull_small2}
   \caption{We qualitatively demonstrate the performance of MAP visibility estimation using the Bat Swing sequence. The normal of the selected patch is shown as a red arrow in the 3D view (left) and projected patch is shown as a red polygon in each image (right). The images with a blue boundary are the views that belongs to the visibility set. The bat occludes the patch and its effect can be seen as a ``shadow" on the visibility set of cameras (left).}
  \label{Fig:VisQ}
\end{figure*}


\subsection{Qualitative Evaluation}
\noindent \textbf{Visibility Boundary.} We qualitatively demonstrate the performance of our MAP visibility estimation using the Bat Swing sequence by illustrating cameras in the visibility set in 3D, and showing the projection of the target patch in the images, as shown in Figure~\ref{Fig:VisQ}. This result shows a clean visibility boundary, showing the occluded views by the baseball bat.  

\noindent \textbf{3D Trajectory Recontruction.}
We generate an initial patch cloud for a selected time instance, and perform forwards and backwards patch tracking, up to 150 frames, for all the sequences summarized in Table~\ref{Table:dataset}. Figure~\ref{Fig:qualtitative} shows the reconstructed trajectories. The reconstructed time instances are color coded. Note that our method can be applied multiple times to different time instances to increase the density of the trajectories. \\
\textbf{Circular movement:} Three people rotate around the person at the center (Figure~\ref{Fig:circular}). This experiment is used to evaluate our method in terms of visibility reasoning\\
\textbf{Volleyball:} Two people play volleyball (Figure~\ref{Fig:ball_play}). We demonstrate an event where motion is fast and occlusion is severe. We are able to reconstruct the trajectories of the ball and players.\\
\textbf{Bat swing:} A person swings a baseball bat. The reconstructed long trajectories can provide a computational basis for sport analytics, capturing subtle motion (Figure~\ref{Fig:baseball}).\\
\textbf{Falling boxes:} A person collides with stacked boxes and the boxes collapse. The scene includes severe occlusion and topological change of the structure (Figure~\ref{Fig:box}).\\
%In this sequence, occlusion is severe because one side of the scene is completely occluded by the stacked boxes. Also the topology of the structure changes significantly where spatial regularity cannot be applied (Figure~\ref{Fig:box}).\\
\textbf{Confetti:} A person throws confetti in the air. 3D reconstruction of such sequences is challenging because of occlusion and appearance changes. Visibility estimation is challenging as the confetti are small and their appearance changes abruptly (Figure~\ref{Fig:confetti}). \\
\textbf{Fluid motion:} We generate turbulent flow in a room using a fan and small confetti (Figure~\ref{Fig:confetti})\footnote{For this result, we turned off geometric consistency by setting $\tau_c = 0$ and $\tau_p = 0$, as the objects are well approximated by planes.}. 
%Our visibility method that leverages a large number of cameras enables us to estimate chaotic motion. This sequence is more challenging than the Confetti sequence because of large displacement, motion blur, and severe interference among the confetti. 
%Our visibility estimation method enables us to reconstruct the chaotic motion of confetti by leveraging the large number of cameras (Figure~\ref{Fig:confetti})\footnote{For this result, we turned off normal cue by setting $\tau = 0$}.


% \begin{comment}
\begin{figure*}[th]
  \centering       
   \subfigure[Circular movement]{\label{Fig:circular}\includegraphics[width=0.32\textwidth]{circle_small}} 
    \subfigure[Volleyball]{\label{Fig:ball_play}\includegraphics[width=0.32\textwidth]{volleyBall_small}}
    \subfigure[Bat swing]{\label{Fig:baseball}\includegraphics[width=0.32\textwidth]{baseball_small}}\\
    \subfigure[Falling boxes]{\label{Fig:box}\includegraphics[width=0.32\textwidth]{box_small}}
    \subfigure[Confetti]{\label{Fig:confetti}\includegraphics[width=0.32\textwidth]{confetti_small}} 
    \subfigure[Fluid motion]{\label{Fig:fluid_motion}\includegraphics[width=0.32\textwidth]{fluid_small}}   
  \caption{We reconstruct 3D trajectories in real world scenes in the presence of significant occlusion, large displacement, and topological change. The color codes the time that trajectory points are reconstructed. Note that each trajectory is individually reconstructed without any spatial or temporal regularization.} 
  \label{Fig:qualtitative}
\end{figure*}
% \end{comment}



\section{Discussion}
We present a method to estimate the time-varying visibility for 3D trajectory reconstruction to leverage large numbers of views. We present novel cues (motion consistency, geometric consistency, and visibility regularization prior) for visibility estimation, and fuse them with the commonly used photometric consistency cue, within a MAP estimation framework. We demonstrate that our algorithm provides a more accurate visibility and, consequently, produces longer and denser 3D trajectories than a baseline using only photometric consistency. Unlike the photometric consistency cue, The motion consistency cue is complementary to the photometric cue, as it does not require the texture and the explicit 3D shape of the target 3D patch. Although the motion consistency cue can be ambiguous, this ambiguity, in practice, usually occurs for the cameras behind the target patch when the whole object body (including the patch) undergoes pure translation; this case is handled well by the geometric consistency of the patch and camera. 

%There are pros and cons of the presented motion consistency cue in term of visibility estimation. It is only applicable if there is motion, and can be ambiguous according to the motion of the object. In practice, the ambiguity occurs for the cameras behind the target patch when the whole object body including the patch undergoes pure translation, which can be handled by the geometric consistency. However, unlike the photometric consistency cue, motion consistency does not require the texture, explicit 3D shape, or precise normal of the target 3D patch, which provides tolerance for the 3D patch reconstruction error. 

A key benefit of our approach is that it does not use any spatial or temporal regularization over the position of the point---the regularization used in our approach is over visibility. This results in ``faithful" reconstruction of 3D point motion, that is not biased or smoothed out by prior models of deformation. The most common cause of failure are imaging artifacts, such as motion blur and saturation. As these kinds of artifacts are unavoidable especially when considering outdoor environments, an important direction of future work is to investigate techniques to re-associate points. 

%; although we can apply the regularization over the position as a post-processing of our method, if it is needed. %For further study, we plan to use these precise measurements of 3D motion to analyze social information and context contained in natural motion among interacting people, which contains highly subtle. 


%For further study, we plan to use these precise measurements of 3D motion to analyze social information and context contained in natural motion among interacting people. Such motion has been considered as a challenging area because the motion often highly subtle and the precision and scale of 3D motion estimation achieved in this paper provide, perhaps for the first time, the opportunity to measure and analyze subtle motion in the interaction of multiple people. 

%We present a method to estimate the time-varying visibility for 3D trajectories reconstruction by leveraging large number of views. We present novel cues (motion consistency, geometric consistency, and visibility regularization prior) for visibility estimation, and fuse them with the common cue photometric consistency, motion-consistency does not require the texture of the 3D patch, and any explicit 3D shape or precise normal. In the end, we fuses a MAP estimate for visibility estimation by fusing various cues, including photo-consistency, motion-consistency, camera-patch relation, and the visibility of proximal cameras. We posit that accurate visibility can increase 3D tracking performance by leverage a large scale cameras, and demonstrate that our MAP based method provide a reliable visibility and our tracking algorithm based on the correct visibility produces accurate, dense, and long 3D trajectories that can better leverage increasing number of viewpoints.



%\textbf{Hyun Soo}: Please describe pros and cons of each visibility cues here.
%As the number of cameras increases, the likelihood of being confused by appearance increases. This makes 3D dynamic reconstruction to fully leverage the benefit of increasing number of cameras difficult. In this paper, we study the role of visibility that influences quality of large scale reconstruction. 

%We demonstrate that optimally estimated visibility facilitates to computationally understand a dynamic scene. The accurate and long trajectories will also be useful to analyze human social behaviors that are often governed by subtle social signals.  


\section*{Appendix}
Proof of Equation~(\ref{Eq:nullspace}): Without loss of generality, we can define the projection matrix as $\mathbf{P} = \left[\begin{array}{cc}\mathbf{I}&\mathbf{0}\end{array}\right]$. Then, Equation~(\ref{eq:ambiguousMotion}) can be rewritten as,
\begin{eqnarray}
\left[\mathbf{X'}(t)\right]_\times \mathbf{X}(t) &=& \mathbf{0} \label{Eq:prop}\\
\left[\mathbf{X'}(t+1)\right]_\times \mathbf{X}(t+1) &=& \mathbf{0} \label{Eq:prop1},
\end{eqnarray}
given $\mathbf{P} = \left[\begin{array}{cc} \mathbf{I} & \mathbf{0} \end{array}\right]$ where $\left[\cdot\right]_\times$ is the skew-symmetric representation of cross product. $\mathbf{X'}(t)$ is linearly proportional to $\mathbf{X}(t)$ because of Equation~(\ref{Eq:prop}) and thus, $\mathbf{X'}(t) = \alpha \mathbf{X}(t)$ where $\alpha$ is a scalar. $\alpha \neq 1$ because then $\mathbf{X'}(t)\neq \mathbf{X}(t)$.

From Equation~(\ref{Eq:transform2}), Equation~(\ref{Eq:prop1}) can be rewritten as,
\begin{eqnarray}
\mathbf{0} &=& \left[\mathbf{A}\mathbf{X'}(t)+\mathbf{a}\right]_\times \left(\mathbf{A}\mathbf{X}(t)+\mathbf{a}\right)\nonumber\\
&=& \left[\mathbf{A}\mathbf{X'}(t)\right]_\times \mathbf{A}\mathbf{X}(t)+\left[\mathbf{a}\right]_\times \mathbf{A}\mathbf{X}(t)\nonumber\\
&+&\left[\mathbf{A}\mathbf{X'}(t)\right]_\times \mathbf{a}+\left[\mathbf{a}\right]_\times \mathbf{a}\nonumber\\
&=&(1-\alpha) \left[\mathbf{a}\right]_\times \mathbf{A}\mathbf{X}(t) \label{Eq:null},
\end{eqnarray}
where $
\left[\mathbf{A}\mathbf{X'}(t)\right]_\times \mathbf{A}\mathbf{X}(t)
=\alpha\left[\mathbf{A}\mathbf{X}(t)\right]_\times \mathbf{A}\mathbf{X}(t)
= \mathbf{0}\nonumber.$
Equation~(\ref{Eq:null}) implies Equation~(\ref{Eq:nullspace}). 

\section*{Acknowledgements}
This material is based upon work supported by the National Science Foundation under Grants No. 1353120 and 1029679. Hanbyul Joo was supported, in part, by the Samsung Scholarship. 

{\footnotesize
\bibliographystyle{ieee}
\bibliography{MyCollection}
}

\end{document}
