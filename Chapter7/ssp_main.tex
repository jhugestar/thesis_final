% !TEX root = thesis.tex

\chapter{Nonverbal Social Signal Prediction}
\label{chapter:prediction}
%
%%%%%%%%%% ABSTRACT
%\begin{abstract}
%   In this paper, we present a new research task to understand human social interactions via computational methods, to ultimately endow machines with the ability of encoding and decoding the social signals humans use. This research direction is essential to make machine that genuinely communicate with humans. We first formulate the social signal prediction problem as a way to model social behaviors by finding the implicit correlations among social signals in a data-driven way. We then present a new dataset, where the full-spectrum of multi-modal social signals (including face, body, and finger motions) are captured in a triadic social interaction scenario. Several baseline approaches and their results are presented to model social interactions in the presented social prediction framework. Finally we discuss the potential of this new task and guide the future directions. 
%\end{abstract}

%%%%%%%%% BODY TEXT

%\section{Introduction}
%The way humans communicate is one of the clearest factors that differentiates us from other animals. Along with verbal language, we use many other channels for communication, including eye contact, facial expression, body gestures, and hand motions, collectively referred to as non-verbal social signals~\cite{Moore13}. Notably, the use of all these channels is important in social interactions, where subtle emotions and intentions are transmitted via the combination of such signals. Endowing machines with such social interaction abilities is an essential goal of Artificial Intelligence (AI) to make machines that can effectively cooperate with humans. This area is related to multidisciplinary research fields covering psychology, natural language processing, affective computing, human-robot interaction, machine learning, and computer vision.


%A way to endow machines with such social skills would be to encode all the rules that humans observe during social communication. Unfortunately, nonverbal interaction is still poorly understood despite its importance in social communication~\cite{Mehrabian67,Mehrabian81,Birdwhistell-1970}, making it hard to formalize rules about how to understand and use social signals. Interestingly, we have recently witnessed a great achievement in Natural Language Processing showing the potential to make machines ``freely" communicate with humans using written language and speech~\cite{young2018recent}. This success has been lead by data-driven approaches leveraging large-scale language datasets and a powerful modeling tool, deep neural networks, to automatically learn the patterns of human verbal communication. Remarkably, these achievements have not made extensive use of the prior knowledge about grammar and the structure of languages that linguists have accumulated over centuries. Motivated by this, we may hypothesize that a similar approach can be applicable in modeling nonverbal communication. 


%However, there exists a fundamental difficulty in building a data-driven nonverbal communication model: the data is extremely rare. In the verbal language domain, words contain the full expressive power to record verbal signals by composition of a handful of discrete symbols. Especially on the Internet, there already exist millions of articles or dialogues which are readily usable for data-driven methods (either supervised or unsupervised). Similarly, speech audio can be readily digitized and large datasets are available. However, for non-verbal signals, how to ``record" or collect these signals is uncertain. Imagine a situation where a group of people are communicating in our daily life. The positions and orientations of individuals, their body gestures, gaze, and facial expressions (among others) are the data we are interested in. Notably, these ``social signals" emitted from all people in the group need to be collected simultaneously to study their correlation and causality. Although there also exist millions of videos where our daily activities---including social interactions---are captured on the Internet, these raw videos cannot be directly used to understand the rules of non-verbal interactions because we have to extract all semantic visual cues (relative location, face, body pose, and so on) from the raw pixels. %Fortunately, measuring human behavior including person detection~\cite{}, body pose estimation~\cite{}, face keypoint detection, hand keypoint estimation, markerless motion captures have all been greatly advanced, enabling us to collect non-verbal signal dataset.

% % Unfortunately, we haven't make the similar success in nonverbal language part, and the major difficulty hindering the progress is the lack of available dataset where computational methods can be tested. 

% Consider a situation where a group of people are communicating in our daily life. They first meet each other, and then they make a special formation to start the communication (either standing or sitting). Someone starts speaking while others may listen, and the speaking turns are circling here and there. Both speaker and listeners use additional channels to transmit additional information which is not fully conveyed by language only, such as hand gestures, facial expression, nodding, eye gaze direction and so on. All these signals and rules presented during the communication is the signals we want to model. But, how can we ``record" or measure these signals so that we can apply the computational methods? In verbal language, words have the full expressive power to record the signals. Especially in the Internet, there already exist tons of literature or dialogues specifying the language signals humans use in our daily life. There is no such complete data for the non-verbal signals. Although there exist millions of video data where our daily activities including social interaction in the Internet, these raw videos cannot be directly used to understand the rules of non-verbal interactions, because we have to extract all semantic visual cues (person location, face, body pose, and so on) out of the raw pixels, which is basically the goal of computer vision area. Notably, these signals should be handled for the visual footage where multiple people are naturally interacting. Due to the person's body orientation and occlusion issues, measuring such signals itself is a challenging problem, despite the great advances of the human detection, pose estimation, reconstruction area in computer vision.

Consider how humans communicate---we use language, voice, facial expressions, and body gestures to convey our thoughts, emotions, and intentions. Such social signals that encode the internal messages of an individual are then sensed, decoded, and finally interpreted by communication partners (see  Figure~\ref{fig:ssp_intro_flow}). One way to computationally model human communication is by finding the mapping between the signals and the conveyed semantics, as in affective computing field~\cite{picard1997affective, picard2003affective,poria2017review} to automatically recognize human emotions. However, how to represent the internal status, either by discrete categories~\cite{ekman1969} or a set of latent dimensions~\cite{osgood1952nature, russell1979affective, plutchik2001nature}, is still controversial due to the fact that we have no way to directly observe the internal status of our mind, and, more importantly, mapping the exposed signals and the status is subjective to observers without an objective way to measure it~\cite{steidl2005all}. 

In our research, we focus on the dynamics between social signals~\cite{pentland2004social} that a subject receives and sends, rather than the mapping between the social signals and their semantics. The advantages of this approach are: (1) we can tackle the problem by investigating the objectively measurable social signal data and (2) it enables us to model subtle details of social communication by considering the original continuous and high-dimensional signal space. By finding the patterns and correlations among social signals, we can computationally model the dynamics of social communications, to ultimately teach a robot how to behave in a social communication. Although similar ideas have been used to model human communication skills in many different fields, including psychology (e.g., study on mimicry)~\cite{bernieri1988synchrony}, virtual agents~\cite{morency2008predicting,morency2010modeling}, human-robot interaction~\cite{huang2014learning}, most of them are studied in a limited range of nonverbal signals, often focusing on faces. 

In this chapter, we aim to model the dynamics (or spatiotemporal correlations) among social signals in a data-driven way. In particular, we take into account a broad spectrum of social signals between individuals, including facial expressions, body gestures, body proximity, and body orientations. While the correlations among a few types of signals of an individual have been already studied (e.g., between speech and facial expressions~\cite{kendon1980gesticulation, mcneill1992hand}, speech and gaze~\cite{griffin2001gaze}, speech and body~\cite{levine2010gesture,marsella2013virtual}), there is no existing work that computationally studies the dynamics of various channels of social signals (e.g., both body gestures and facial expressions) transmitted among individuals. The main obstacle in pursuing the direction is the lack of available dataset or measurement technique. The sensing and measuring techniques presented in this thesis based on the Panoptic Studio (Chapters~\ref{chapter:system}, \ref{chapter:trajectory}, \ref{chapter:mocap}, \ref{chapter:totalcapture}) allow us to enable this study for the first time, and we leverage the Haggling sequences (described in Chapter~\ref{chapter:dataset}) captured from hundreds of participants for this direction.

\begin{figure}[t]
	\includegraphics[width=\linewidth]{ssp_fig/socialcomm}
	\caption{Humans communicate by transmitting signals using several ``display ports'' such as voice, body motion, and facial expressions. Such signals are then sensed, decoded, and interpreted by others. We hypothesize that there exists a strong link between the signals exchanged during the social interaction, and aim to computational model the dynamics of them.}
	\label{fig:ssp_intro_flow}
\end{figure}

We first formulate a social signal prediction task to model the dynamics of social communication as a function of input and output social signals (see Figure~\ref{fig:ssp_intro}). In this task, we consider a target individual who receives a set of social signals from others and emits response signals as output. We hypothesize that the social behavior of the target person can be learned by finding the patterns between these signal flows, by which machines know how to behave given the signals humans produce. Importantly, we aim to include as many channels of signals as possible to study the correlation between various channels of social signals. However, directly investigating the dynamics of all spectrum social signals provided by our measurement method is challenging due to the high complexity of the motion space, requiring a much larger scale of data. We thus simplify the problem by focusing on predicting lower dimensional, yet important, output signals---speaking status and social formations---emitted by the target person, while still considering broader channels including body motion and facial expressions as input. We found that this approach still provides an important opportunity to computationally study various channels of interpersonal social signals. In this chapter, we explore various issues in pursuing this research direction by discussing problem formulation, data representation, and evaluation metric. Results on our baseline social signal prediction models, implemented by neural networks, demonstrate the strong predictive property among social signals and also allow us to quantitatively compare the relative influence among them. To this end, we further discuss the future research direction to ultimately mimic the social behavior of humans.



% This ability is also strongly related to predicting social signals among . direction is based on the hypothesis that a human has a similar dynamics model for social signaling. When we communicate, we anticipate that communication partners behave in a certain way, and not be surprised by them; for example, when we are talking to others we expect eye contact, head nodding, and back-channel utterances (e.g., "yes" or "uh-huh") from the communication partners. Similar ideas have been used to model human communication skills in many different fields, including psychology (e.g., study on mimicry)~\cite{bernieri1988synchrony}, virtual agent~\cite{morency2008predicting,morency2010modeling}, Human-robot interaction~\cite{huang2014learning}.



%
%Interestingly,  We do not expect the communication partner performs random or unexpected behaviors suddenly. This is also closely related to the screening and diagossing autism spectrum disorder~\cite{stone2000brief, mathys2013beyond}. For example, during the test, the examiner monitor the behavior of the subject in an interactive, play-based tests,  assessing  social-communicative behaviors of the subject. These test are based on the assumption that there exist socially expected behaviors among communication for healthy subjects, and measure how the behavior of the subject is different from the expected healthy feedback.  


%We study various types of human behaviors including proxemics, F-formation, and body gestures, and speaking timing. In this paper, we introduce several baseline results for the social signal prediction task. 

%In summary, we present:
%\begin{itemize}
%    \item A new dataset of triadic interactions with full spectrum capture of social signals. 
%    \item A formal definition of the problem for social signal prediction
%    \item Several baseline initial outputs in several scenarios
%\end{itemize}
%
%\begin{figure}
%	\centering
%	\includegraphics[width=\linewidth]{ssp_fig/intro}
%	\caption{We aim to build social artificial intelligence by building a model to learn the correlation between the input and output social signals. (Left) An example scene of RAPID-ABC test to detect autism spectrum disorder ~\cite{mathys2013beyond} and (Right) An illustration to describe social signal prediction task.}
%	\label{fig:ssp_intro}
%\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{ssp_fig/intro}
%	\includegraphics[width=\linewidth]{ssp_fig/ssp_def}
	\caption{We aim to learn the correlation between the input signals $\mathbf{X}$ an individual receives and the output social signals $\mathbf{Y}$ the individual emits. The goal of social signal prediction is to regress a function for these in a data-driven manner.}
	\label{fig:ssp_intro}
\end{figure}

% 1) We have 

% 2) General notations and social signal prediction problem. We formulate the prediction problem. 

% 3) We start from a simple signals. Proxemics and F-formation, and we make extension to more complicated signals, body and faces in social situation. We define the problem based on the difficulty. 

%     - position only
%     - body orientation (low freq.)
%     - face orientation
%     - body skeleton
%         - just better vis. from formation
%         - speaking / non-speaking (conditioned)
%         - social dynamics / social sync (nodding, synchrony, realistic?)
        
%     - Face/Hands
%         - just better vis. from formation + body
%         - speaking / non-speaking
%         - social dynamics / social sync (nodding, synchrony, realistic?)


% 5) We built a way to evaluate our method, as follows. 

% 1. L2 error
% 2. Classification error
% 3. Adding distribution and randomness (VAE?)

% 6) Conclusion and Future direciton




% Social interaction  long history but poor knowledge. Harder to define by manually defined models than language model. Language has grammar, what about nonverbal cues? We have seen the advance of language (and chatbot as the output). Limited measurment tool is the problem.  Language has tons of data in the web. No need to measure since verbal languages are directly mapped and recorded by letters. What about non verbal cues?

% A major advanced in facial expression (coding system, interaction). Body is limited, especially multi people's body, which are more commonly observable in our daily life. But vision made major advances. There exist robust tools. Now this problem is used in a practical level. We need to consider how to realize the social interaction model based on this tools. 


% However, It is not easy to computationally model social interaction. Extremely limited previous work. We may think it by using the analogy of language model or chatbot. We found (1) data is important, (2) nerual model is crucial. This paper presentned such direction for the first time. new DB can drive new research direction advances. MNist, Imagenet, 3D shapenet. 3D human DB, faceNet, and so on. 

% Thus, 

% 1) We have collected a sufficient amount of DB to study this assuming a specific (but still realistic) social situations. 

% 2) General notations and social signal prediction problem. We formulate the prediction problem. 

% 3) We start from a simple signals. Proxemics and F-formation, and we make extension to more complicated signals, body and faces in social situation. We define the problem based on the difficulty. 

%     - position only
%     - body orientation (low freq.)
%     - face orientation
%     - body skeleton
%         - just better vis. from formation
%         - speaking / non-speaking (conditioned)
%         - social dynamics / social sync (nodding, synchrony, realistic?)
        
%     - Face/Hands
%         - just better vis. from formation + body
%         - speaking / non-speaking
%         - social dynamics / social sync (nodding, synchrony, realistic?)


% 5) We built a way to evaluate our method, as follows. 

% 1. L2 error
% 2. Classification error
% 3. Adding distribution and randomness (VAE?)

% 6) Conclusion and Future direciton

% We start from indivdiual's multi modal signals, and show several noticeable component we can explore. 

% 4) Social formation. Simple but strong correlation among people which  well studied in previous work. We see wheather this social aspect can be learnt by machine. 

% Comparison to proxemics. 
% Evalution: body location, body orientation, face orientation. Orientation vs speaking GT?

% 5) More complicated singnals are studied. For example, body 2 body. However, the diverse is huge and motion are subtle, and less noticeable. So we focused on some cues by defining evalution tools. 


% Evalutation: How do we know it is working? Number may not be important. What else can we notices? It is hard, because we can feel but cannot define. 

% 1) Location + orientations 
% 2) orientation vs speaking
% 3) turn taking?

% 4) dynamics? nodding? Speed? human study?



\section{Related Work}

\textbf{Psychology:}
Due to its importance in social communication, nonverbal cues have received significant attention in psychology. Work in this area is often categorized into diverse subfields including Proxemics, Kinesics, Vocalics, and Haptics. In our work, we focus only on Proxemics and Kinesics, which are closely related to visual cues. Hall first introduced the concept of Proxemics to describe how humans use their personal space during communications~\cite{Hall66}, and Kendon studied the spatial formations and orientations established when multiple people communicate in a common space (named F-formation)~\cite{kendon90}. Facial expression, in particular, has received lots of attention by researchers since the pioneering work of Charles Darwin~\cite{Darwin-1872}. Ekman studied the relation between emotions and facial expressions, and presented the  Facial Action Code System (FACS) by introducing a system to describe facial expressions using combinations of atomic units (Action Units)~\cite{ekman1977facial}. Since then, this system remains as the de-facto standard to annotate and measure facial expressions, and has had a broad impact across many fields. Compared to the face, body gestures remain relatively unexplored even though research has substantiated the importance of body language in communication~\cite{Gelder09, Moore13, Meeren-2005, Aviezer-2012}. In spite of the efforts of many researchers in diverse fields, little progress has been made in understanding nonverbal communications, and the approaches proposed several decades ago are still the most widely used methods available~\cite{Moore13}.


%\textbf{Virtual Agent and HRI:}





\textbf{Social Signal Processing:}
There has been increasing interest in studying nonverbal communication using computational methods~\cite{vinciarelli2009social, vinciarelli2012bridging}. Analyzing facial expression has been a core focus in the vision community ~\cite{ChuDC13, Torre15, shan2009facial}. Many other methods to automatically detect social cues from photos and videos have also been proposed, including F-formation detection~\cite{setti2015f}, recognizing proxemics from photos~\cite{yang2012recognizing}, detecting attention~\cite{Fathi-2012}, recognizing emotions by body pose~\cite{schindler2008recognizing}, and detecting social saliency~\cite{park20123d}. The affective computing field has been growing rapidly, where computer vision and other sensor measurements are used with machine learning techniques to understand human emotion, social behavior, and roles~\cite{picard1997affective, picard2003affective,poria2017review}. 

\textbf{Forecasting human motion:}

Predicting or forecasting human motion is an emerging area in computer vision and machine learning. Researchers propose approaches for predicting a pedestrian's future trajectory~\cite{kitani2012activity} or forecasting human interaction in dyadic situations~\cite{huang2014action}. More recently, deep neural network is used to predict future 3D poses from motion capture data~\cite{mnih2012conditional, Fragkiadaki_2015_ICCV, jain2016structural}, but they focus on periodic motions such as walk cycles. Recent work attempts to forecast human body motion in the 2D image domain~\cite{walker2016uncertain, villegas2017learning}. A few efforts address trajectory prediction in social situations~\cite{helbing1995social, alahi2016social, gupta2018social}. 

%Holden's work
%Graphics Jehee's
%Sergey Levin's work
%
%\textbf{Social Signal Dataset:}
%How to measure and collect nonverbal signal data is important to pursue a data-driven approach for our goal. However, only few datasets contain socially interacting group motion~\cite{alameda2016salsa, mccowan2005ami, lepri2012connecting, rehg2013decoding}. The scenes in these datasets are often in a table setup, limiting free body movement and capturing the upper-body only. There are datasets that capture more freely moving multiple people (e.g., cocktail party)~\cite{Zen-10, Cristani-11, farenzena2009social}, but these only contain location and orientation measurements for the people, introduced to study the social formation only. Datasets providing rich 3D body motion information captured with motion capture techniques exist, but they contain single subjects' motion only\cite{gross2001cmu, h36m_pami, sigal2010humaneva}. More recently, however, full body motion data of interacting groups using a large number of camera system was proposed for social interaction capture~\cite{Joo-15}. This work shows a potential in collecting a large scale social interaction data without the usual issues caused by wearing a motion capture suit and markers.

\textbf{Measuring Nonverbal Signals in Imagery:} Detecting human bodies and keypoints in images has advanced greatly in computer vision. There exist publicly available 2D face keypoint detectors~\cite{baltruvsaitis2016openface}, body pose detectors~\cite{cao2017realtime, Wei2016, Newell-16}, and hand pose detectors~\cite{simon2017hand}. 3D motion can be obtained by markerless motion capture in a multiview setup~\cite{Gall-09,Liu-2013,Elhayek-15, joo2017panoptic, joo2018}, by RGB-D cameras~\cite{Shotton2011,Baak2011}, or even by monocular cameras~\cite{Ramakrishna2012,Bogo2016,martinez2017simple,zhou2017towards,Moreno-noguer2017,mehta2017monocular}. Recently, methods to capture both body and hands have also been introduced~\cite{MANO:SIGGRAPHASIA:2017,joo2018}. 

\input{Chapter7/ssp_method_def}
%\input{Chapter7/ssp_method_db}
\input{Chapter7/ssp_method_pred}
%\input{Chapter7/ssp_method_details}
\input{Chapter7/ssp_result}
\input{Chapter7/ssp_discussion}


