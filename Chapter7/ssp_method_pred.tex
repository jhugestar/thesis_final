% !TEX root = ../thesis.tex

\section{Social Signal Prediction in Haggling Scenes}
We use our Haggling scenario as an example to computationally model triadic interaction.  In this section, we formally define the input and output signals used in our modeling. We then introduce two social signal predicting problems, predicting speaking status and predicting social formation, describing their problem definitions and implmentation details. Note that in our modeling scenario, we focus on estimating the target person's concurrent signals by taking other individual's signals as input  as defined in Equation~\ref{equation:F_ours}, rather than forecasting the future signals, although it can be handled in the similar framework .  

\subsection{Notation}

Our mesurement method in the Panoptic Studio reconstructs 3D body motion $\mathbf{J}(t)$ and 3D face motion $\mathbf{F}(t)$ for each individual at each time $t$\footnote{We do not use the hand motion measurement due to the occasional failures in challenging hand motions (e.g., when both hands are close each other), making it hard to train our model. We, however, believe this cue plays an important role in social interaction, which needs to be considered in future direction.}. From this measurement, we additionally compute the body orientation $\boldsymbol{\theta}(t)$ and face orientation $\boldsymbol{\phi}(t)$ by finding the 3D normal direction of torso and face. We describe the details below.

%Some figures to explain this
\paragraph{Body Motion:} We follow the body motion representation of the work of Holden et al.~\cite{holden2016deep}, representing a body gesture at a frame as a 73-dimensional vector, $\mathbf{J}(t) \in \mathbb{R}^{73}$. This representation is based on the skeletal structure of CMU Mocap dataset~\cite{gross2001cmu} with 21 joints (63 dimensions), along with the projection of the root joint (center of the hip joints) on the floor plane (3 dimensions), relative body locations and orientations represented by the velocity values of the root (3 dimensions), and footstep signals (4 dimensions).  The orientations are computed only on the $x$-$y$ plane with respect to the $y$-axis, and location and orientation contain the velocity values from the previous frame rather than the absolute values, following previous work~\cite{jain2016structural, holden2016deep}. The body joint locations (the first 63 dimensions of $\mathbf{J}(t)$) represents the body motion in the person-centric coordinate (root joint is at the origin and body is facing the $z$ direction). We perform a retargeting process to convert our original 3D motion data from the Panoptic Studio (following the skeleton definition of COCO dataset~\cite{coco-14}) to this motion representation with a fixed body scale. Thus, in our final motion representation, identify specific cues such as heights or lengths of limbs are removed and only motion cues are kept.

\paragraph{Face Motion:}  For the face signal, we use a subpart (initial 5 dimensions) of the facial expression parameters of Adam model~\cite{joo2018,cao2014facewarehouse}, because we found the remaining dimensions have an almost negligible impact on our reconstruction quality. Thus, face motion at a time instance is represented by a 5-dimensional vector, $\mathbf{F}(t) \in \mathbb{R}^{5}$.
 
\paragraph{Position and Orientation:} We use 2D unit vectors for the face orientation and body orientation ($\boldsymbol{\theta}(t)$ and $\boldsymbol{\phi}(t) \in \mathbb{R}^{2} $ ), defined on the $xz$-plane ignoring the values in $y$ axis. For the position, we use the root joint of the body, ignoring the values in $y$ axis, and thus $\mathbf{x}(t) \in \mathbb{R}^2$. Note that we use the global values for this position and orientations to study social formations among interacting individuals, different from the realtive values in defining $\mathbf{J}(t)$.  Thus, the status of an individual at a frame in social formation prediction is represented by a 6-dimensional vector, $[\mathbf{x}(t), \boldsymbol{\theta}(t), \boldsymbol{\phi}(t) ] \in \mathbb{R}^6$.

\paragraph{Voice and Speaking Status:} The voice data $V(t)$ of each individual is also recorded by wireless microphones assigned to each individual. From the audio signal, we manually annotate a binary speaking label $\mathbf{S}(t) \in \{0,1\}$ describing whether the target subject is speaking (labelled as $1$) or not speaking (labelled as $0$) at time $t$.\\%In summary we measure the following signals for each individual:
%\begin{equation}
%[ \mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\phi}, \mathbf{J}, \mathbf{F}, \mathbf{H}, \mathbf{V}, \mathbf{S} ].
%\label{equation:measurement}
%\end{equation}
\mbox{ }\\
By leveraging the various behavioral cues measured in the Haggling scenes, we model the dynamics of these signals in a triadic interaction. The objective of our direction is to regress the function defined in Equation~\ref{equation:F_ours}. To further constrain the problem we assume that the target person is the seller positioned on the left side of the buyer, and as input we use the social signals of the buyer ($\mathbf{X}^1$) and the other seller ($\mathbf{X}^2$). Based on our social signal measurements, the input and output of the function are represented as,
\begin{equation}
\begin{gathered}
\mathbf{Y} = [ \mathbf{x}^0, \boldsymbol{\theta}^0, \boldsymbol{\phi}^0, \mathbf{J}^0, \mathbf{F}^0, \mathbf{S}^0 ]\\
\mathbf{X}^i = [ \mathbf{x}^i, \boldsymbol{\theta}^i, \boldsymbol{\phi}^i, \mathbf{J}^i, \mathbf{F}^i, \mathbf{S}^i ],
\end{gathered}
\end{equation}
where we use the superscript 0 to denote the social signals of the target subject (the output of social signal prediction). 

% \begin{figure*}
% 	\centering
% 	\includegraphics[width=\textwidth]{fig/ssp_notation3}
% 	\caption{Locations and orientations of an interacting group can be represented by different coordinate systems. (a) Locations and orientations in a world coordinate, (b) Locations and orientations in a person centric coordinate by person $\mathbf{P}_1$, (c) A location of $\mathbf{P}_1$  in a normalized coordinate by a pair of people $\mathbf{P}_2$ and $\mathbf{P}_3$, (d) An orientation of $\mathbf{P}_1$ in a normalized coordinate by a pair of people  $\mathbf{P}_2$ and $\mathbf{P}_3$. } 
% 	\label{fig:ssp_notation1}
% \end{figure*}

\subsection{Predicting Speaking}
\label{subsection:ssp_pred_speak}
As the first task, we predict whether the target subject is currently speaking or not, that is $\mathbf{S}^0$.  This is a binary classification task and relatively easier to train. We first study the correlation between the speaking signal and the target person's own social signals, either body motion or facial motion, or both. We expect this correlation is stronger than the link between individuals. We then compare it with the performance of a classifier that uses the conversational partner's social signals. More specifically, a function $\mathcal{F}_{J0\rightarrow S0}$ uses the target person's own body motion $\mathbf{J}^0(t_0:t)$ to predict the speaking signal:
\begin{gather}	
\mathbf{S}^0(t_0:t) = \mathcal{F}_{J0\rightarrow S0} \left( \mathbf{J}^0(t_0:t) \right),
\label{eq:speaking_0}
\end{gather}
and similarly,
\begin{gather}	
\mathbf{S}^0(t_0:t) = \mathcal{F}_{F0\rightarrow S0} \left( \mathbf{F}^0(t_0:t) \right),\\
\mathbf{S}^0(t_0:t) = \mathcal{F}_{(F0, J0)\rightarrow S0} \left( \mathbf{F}^0(t_0:t) , \mathbf{J}^0(t_0:t)\right),
\label{eq:speaking_0_facebody}
\end{gather}
where $\mathcal{F}_{F0\rightarrow S0}$ uses the target person's own face motion, and $\mathcal{F}_{(F0, J0)\rightarrow S0}$ uses both. 

We compare the performance of these functions with the functions that takes the signals from a communication partner, the other seller:
\begin{gather}	
\mathbf{S}^0(t_0:t) = \mathcal{F}_{J2\rightarrow S0} \left( \mathbf{J}^2(t_0:t) \right),\\
\mathbf{S}^0(t_0:t) = \mathcal{F}_{F2\rightarrow S0} \left( \mathbf{F}^2(t_0:t) \right),\\
\mathbf{S}^0(t_0:t) = \mathcal{F}_{(F2, J2)\rightarrow S0} \left( \mathbf{F}^2(t_0:t), \mathbf{J}^2(t_0:t) \right),
\label{eq:speaking_1}
\end{gather}
where the functions use body cues, face cues, and both cues, respectively. 

Equipped with this framework, we can computationally study the correlation of different types of social signals from the same individual and across individuals. We hypothesize that the correlations of the target person's own signals are stronger than the correlations among the interpersonal signals, while we still expect that there exists a clear link in the latter case. We verify this in our experimental results.

%the social signals oindividual dynamics of the social signals is stronger than correlation between the speaking and the social signal body motion should be stronger than the speaking of the target and other people's body motions. But we still presume that there exists a clear correlation in building the function ~\ref{eq:speaking_1}; for example, if another person is talking we can guess the target person is not speaking due the ``turn-taking" implicitly followed during a social conversation. % This study is interesting because it demonstrates the existence of social correlation among social signals.

\paragraph{Implementation Details.}
We use four 1-D convolutional layers (the first three networks have 128, 256, and 512 dimensional output features respectively and filter widths are 7 for all of them) for the network where the last layer has $1\times1$ convolutions, followed by a sigmoid activation layer. Our model does not require a fixed window size for the input, but we separate input data into small clips with a fixed size (denoted by $f$) for the efficiency in training. During testing time, our models can be applied to the input of arbitrary length. We use $f=120$, about 4 seconds, for the input window size. The feature dimension of the input of our network is the same as the concatenation of face motion and body motion (78 dimensions), but if fewer cues are used (e.g., face only or body only) we mask out the unused channels by keeping the same network structure for the fair comparison. We use the stochastic gradient descent algorithm Adam~\cite{kingma2014adam}. Dropout is also applied with a retention probability of 0.25.  

%The basic architecture follows the work of Holden et al.~\cite{holden2016deep} that demonstrates a compelling result for the mapping between a 2D trajectory and human motions. We modify the network architecture based on the input and output dimensions. 

%The network architecture is shown in the first row of Figure~\ref{fig:architectures}.
%\label{section:body2speak}
%The input of this network is the body motion of the other seller in the haggling sequence, which is a $f \times 73$ matrix, where $f$ is the number of frames of an input sequence. Here, we ignore the buyer's body motion because often little movement is observed from the buyers.  

\subsection{Predicting Social Formations}
We predict the location and orientations of the target person. This problem is strongly related to Proxemics~\cite{Hall66} and F-formation~\cite{kendon90}, illustrating how humans use their space in social communications.
\begin{gather}	
 \mathbf{Y}_p (t_0:t) = \mathcal{F}_p \left( \mathbf{X}_p^1(t_0:t), \mathbf{X}_p^2(t_0:t) \right),
 \label{eq:pred_formation}
\end{gather}
where $\mathbf{Y}_p$ and $\mathbf{X}_p^i$ contains global location and orientation signals $[\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\phi} ]$ for the target subject and others.
Note that we only consider the positions and orientations on the ground plane (in 2D), ignoring the height of the subjects. Thus $\mathbf{x} \in \mathbb{R}^2 $ representing $x$ and $z$ coordinate of the subjects. We use a 2D unit vector to represent the orientations $\theta \in \mathbb {R}^2$ and face orientation $\phi \in \mathbb{R}^2$, because the angle representation has a discontinuity issue when wrapping around $2\phi$ and $-2\phi$. In summary, $\mathbf{X}^i(t)$ and $\mathbf{X}^i(t)$ are a $6 \times N$ dimensional matrix where $N = t- t_0$ is the size of the temporal window. This prediction problem is intended to see whether the machine can learn how to build a social formation to interact with humans~\cite{vazquez2017towards}. %We Social formation is one of the most noticeable social properties with strong correlation, allowing us to easily evaluate the performance.


\paragraph{Implementation Details.}
We use a 2D vector for the position and 2D unit vectors for the face orientation and body orientation, defined on the $xz$-plane. Thus the status of an individual at a frame in social formation prediction is represented by a 6-dimensional vector.


The input of the social formation network is the concatenation of global position and orientation information of the communication partners, represented by a $f \times 12$ matrix.  We use a simple autoencoder structure, as shown in the second row of Figure~\ref{fig:architectures}.


%To implement this, we use a simple 1-D temporal convolutional neural network to model $\mathcal{F}_p$. The network is trained with a window of frames $N=120$ (corresponding to 4 seconds), but arbitrary length of input can be used in testing time since our network is fully convolutional. We use a simple encoder-decoder network. The encoder has  3 convolutional layers followed by dropout (0.25) and RELU, and 1D pooling layer (with stride 2). The decoder uses a single transposed convolution layer. We use L2 loss function with a L1 regularization term (with weight 0.1). We simply use the global position and orientations with standardization, rather than any local or relative coordinate.
