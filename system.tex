% !TEX root = thesis.tex

%
%
%\chapter{The Panoptic Studio: A Massively Multiview Capture System}
%\section{Modularized Hardware Design}
%\subsection{Structure}
%\subsection{Architecture}
%\section{Calibration}
%\subsection{Temporal Calibration}
%\subsection{Spatial Calibration}
%\section{Capture Procedures}
%
%\chapter{Methods For Social Interaction Capture}
%\section{Dense Trajectory Stream Reconstruction}
%\section{Markerless Social Interaction Capture}
%\subsection{Body}
%\subsection{Face}
%\subsection{Hand}
%\section{Motion Refinement to Capture Subtle Details (Proposed)}
%\section{Panoptic Studio Dataset}


\chapter{The Panoptic Studio}%: A Massively Multiview System}
\label{chapter:system}

There are principal challenges in capturing kinesic signals between individuals in a group: (1) social interactions have to be measured over a volume sufficient to house a dynamic social group, yet subtle details of the motion where important social signals are embedded must be captured; (2) strong occlusions emerge functionally in natural social interactions (e.g., people systematically face each other while interacting, bodies are occluded by gesticulating limbs); (3) human appearance and configuration variation is immense; and (4) social signaling is sensitive to interference---for instance, attaching markers to the face or body, a pre-capture model building stage, or even instructing each individual to assume a canonical body pose during an interaction, primes the nature of subsequent interactions. 

The Panoptic Studio is developed to overcome these challenges. The system has a large geodesic sphere structure with a 5.49m diameter, with heterogeneous sensors on its surface including 480 VGA cameras, 31 HD cameras, and 10 RGB+D sensors. The core principle and motivation in building this system is to obtain as much measurement as possible to have minimum assumptions about the scenes. We found that the large number of views of the system greatly releases all the aforementioned sensing challenges, and also enables us to use relatively ``weak" perceptual processes from the large number of views rather than a complicated method with strong assumptions about the scenes.
 
This chapter covers the various hardware issues in building the Panoptic Studio, including structure design, architectures, synchronization, and spatial calibration.  

%do not introduce any kind of sight restriction on the subjects
%Why not using just 100 HD instead of this combination?)
%Modularized design is the key (same number of cameras, HD on the center). 

% Modularized design, VGA, HD, Kinect
\section{Structural Design}
	% Add optimization routine
	The physical frame of the studio is a variant of a face-transitive solid called a truncated pentagonal hexecontahedron. This particular structure was selected because it has among the largest number of transitive faces of any geodesic dome~\cite{Williams1979}. The transitivity of the faces enables the modular architecture, and ensures that the structure remains easy to upgrade and customize with different panels of the same configuration. The structure has a diameter of 5.49m and a total height of 4.15m. The floor of the dome is 1.40m below the center to increase access to the edges, as shown in Figure~\ref{fig:domeFigure}. In all, the structure consists of 6 pentagonal panels, 40 hexagonal panels, and 10 trimmed base panels. 
	
	Our design was modularized so that each hexagonal panel houses a set of 24 VGA cameras. To determine the placement of the VGA cameras, we initialized their positions by tessellating the hexagon face into 24 triangles and using this initialization to define a 3-neighborhood structure shown in the bottom right illustration of Figure~\ref{fig:domeFigure}. Using this neighborhood structure and the initialization we determine the placement of the cameras over the geodesic dome by minimizing the difference in angles between all neighbors of every camera,
	
	{\small
		\begin{equation}\nonumber
		\{\theta_{ij}\}^* = \arg \min_{\{\theta_{ij}\}} \sum_{p=1}^P \sum_{i=1}^{N} \sum_{j \in \mathcal{N}(i)}  \sum_{k \in \mathcal{N}(i) \neq j}  (r(\theta_{ij}|p)-r(\theta_{ik}|p))^2 ,
		\end{equation}
	}where $P=20$ is the number of panels, $N=24$ is the number of cameras in each panel, $\mathcal{N}(\cdot)$ is the neighborhood of a camera, $r(\cdot|p)$ is a function transforming the angle on a reference panel to the $p$-th panel. The cameras sample the span of the vertical axis of the space and sample $48.71^\circ$ of the horizontal axis. With this distribution, the minimum baseline between any VGA camera and its nearest three neighbors is 21.05cm. 
		
	The 31 HD cameras are installed at the center of each hexagonal panel, and 5 projectors are installed at the center of each pentagonal panel\footnote{Note that no sensors are installed on some panels (e.g., ceiling panels occluded by lights).}. Additionally, a total of 10 Kinect v2 RGB+D sensors are mounted at heights of 1 and 2.6 meters, forming two rings with 5 evenly spaced sensors each. The interior and exterior of our system are shown in Figure~\ref{fig:domeFigure}, and components of our system is shown in Figure~\ref{fig:domeEquipment}
	
	\begin{figure}
		\centering       
	%	\subfigure{\label{fig:domeFigure1}\includegraphics[trim=400 0 150 0,clip,width=0.45\linewidth]{imgs/Dome_outside}} 
		\includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/Dome_photo_0815}
		\includegraphics[trim=0 0 100 0,clip,width=0.48\linewidth]{figures/DomeFigure2} 
		\includegraphics[trim=230 50 400 50,clip,width=0.48\linewidth]{figures/DomeFigure3}
		%	\subfigure[TrajCompares]{\label{Fig:asso_traj}\includegraphics[width=0.5\textwidth]{img/trajCompare}}   
		\caption{The studio structure. (Top) The exterior of the dome with the equipment mounted on the surface. (Middle) The interior of the dome. VGA cameras are shown as red circles, HD cameras as blue circles, Kinects as cyan rectangles, and projectors as green rectangles. (Bottom left) The panels are designed to ensure interchangeability. (Bottom right) Optimized camera positions to ensure uniform angles with respect to the dome center between each camera and all its neighbors (e.g., Camera $i$ is a neighbor of Camera $j$).} 
		\label{fig:domeFigure}
	\end{figure}
		 %Naturally, there is some variation in this number due to the physical imprecision of mounting.
	%Placement of cameras.
	%Williams, Robert (1979). The Geometrical Foundation of Natural Structure: A Source Book of Design. Dover Publications, Inc. ISBN 0-486-23729-X. (Section 3-9)
	% \noindent \textbf{Acquisition Hardware.} The dome has twenty panels, each with 24 VGA cameras and one HD camera, for a total for 480 VGA cameras and 16 HD cameras. The sensors are arranged to lie uniformly around the twenty panels. 
	
	\begin{figure}
	\centering       
	%	\subfigure{\label{fig:domeFigure1}\includegraphics[trim=400 0 150 0,clip,width=0.45\linewidth]{imgs/Dome_outside}} 
	\includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/Equipment}
	\caption{Sensors and equipments installed in the Panoptic Studio. } 
	\label{fig:domeEquipment}
\end{figure}
\section{System Architecture}
%which consists of 480 VGA cameras, 31 HD cameras, 10 Kinect v2 RGB+D sensors, and 5 DLP projectors. 
Figure \ref{fig:architecture} shows the architecture of our system. The 480 cameras are arranged modularly with 24 cameras in each of 20 standard hexagonal panels on the dome. Each module in each panel is managed by a Distributed Module Controller (DMC) that triggers all cameras in the module, receives data from them, and consolidates the video for transmission to the local machine. Each individual camera is a global shutter CMOS sensor, with a fixed focal length of 4.5mm, that captures VGA ($640\times480$) resolution images at 25Hz. 

Each panel produces an uncompressed video stream at 1.47 Gbps, and thus, for the entire set of 480 cameras the data-rate is approximately 29.4 Gbps. To handle this stream, the system pipeline has been designed with a modularized communication and control structure. For each subsystem, the clock generator sends a frame counter, trigger signal, and the pixel clock signal to each DMC associated with a panel. The DMC uses this timing information to initiate and synchronize capture of all cameras within the module. Upon trigger and exposure, each of the 24 camera heads transfers back image data via the camera interconnect to the DMC, which consolidates the image data and timing from all cameras. This composite data is then transferred via optical interconnect to the module node, where it is stored locally. Each module node has dual purpose: it serves as a distributed RAID storage unit\footnote{Each module has 3 HDDs integrated as RAID-0 to have sufficient write speed without data loss, totaling 60 HDDs for 20 modules.} and participates as a multi-core computational node in a cluster. All the local nodes of our system are on a local network on a gigabit switch. The acquisition is controlled via a master node that the system operator can use to control all functions of the studio.

%The 31 HD cameras are installed at the center of each panel, and the HD cameras capture the scene at 29.97 Hz with ($1920{\times}1080$) resolution. 
Similar to the VGA cameras, HD cameras are modularized and each pair of cameras are connected to a local node machine via SDI cables. Each local node saves the data from two cameras to two RAID storage units respectively.

%Additionally, a total of 10 Kinect v2 RGB+D sensors are mounted at heights of 1m and 2.6 meters, forming two rings with 5 evenly spaced sensors each. 

Each RGB+D sensor is connected to a dedicated capture node that is mounted on the dome exterior. To capture at rates of approximately 30 Hz, the nodes are equipped with two SSD drives each and store color, depth, and infrared frames as well as body and face detections from the Kinect SDK. A separate master node controls and coordinates the 10 capture nodes via the local network.

An example scene captured by all the sensors are shown in the Figure~\ref{fig:panopticviews}. The data size of a minute of data is about 600 GB. 

\begin{figure}[t]
	\includegraphics[width=\linewidth]{figures/Dome_architecture_0815}
	%\includegraphics[width=\linewidth]{images/Design6.pdf}
%	\vspace{-0.2in}
	\caption{Modularized system architecture. The studio houses 480 VGA cameras synchronized to a central clock system and controlled by a master node. 31 synchronized HD cameras are also installed with another clock system. The VGA clock and HD clock are temporally aligned by recording them as a stereo signal. 10 RGB-D sensors are also located in the studio. All the sensors are calibrated to the same coordinate system.}
	\label{fig:architecture}
\end{figure}

\section{Temporal Calibration for Heterogeneous Sensors}
Synchronizing the cameras is necessary to use geometric constraints (such as triangulation) across multiple views. In our system, we use hardware clocks to trigger cameras at the same time. Because the frame rates of the VGA and HD cameras are different (25 fps and 29.97 fps respectively) we use two separate hardware clocks to achieve shutter-level synchronization among all VGA cameras, and independently among all HD cameras. To precisely align the two time references, we record the timecode signals generated from the two clocks as a single stereo audio signal, which we then decode to obtain a precise alignment at sub-millisecond accuracy. 

Time alignment with the Kinect v2 streams (RGB and depth) is achieved with a small hardware modification: each Kinect's microphone array is rewired to instead record an LTC timecode signal\footnote{As a result of this modification, microphone output on the Kinects is therefore discarded. More details about this hardware modification are available upon request.}.  This timecode signal is the same that is produced by the genlock and timecode generator used to synchronize the HD cameras, and is distributed to each Kinect via a distribution amplifier. We process the Kinect audio to decode the LTC timecode, yielding temporal alignment between the recorded Kinect data---which is timestamped by the capture API for accurate relative timing between color, depth, and audio frames---and the HD video frames. Empirically, we have confirmed the temporal alignment obtained by this method to be of at least millisecond accuracy.
%, with the RGB sensor being a rolling shutter with a scan time of approximately 25ms.

\begin{figure}
	\centering
	\includegraphics[height=0.12\linewidth]{figures/calib_tent_hd}
	\includegraphics[height=0.12\linewidth]{figures/calib_tent_kinect}
	\includegraphics[height=0.12\linewidth]{figures/calib_tent_vga}\\
	\includegraphics[trim=50 0 50 0,clip,width=0.4\columnwidth]{figures/domeCalib/38}  
	\includegraphics[trim=50 0 50 0,clip,width=0.4\columnwidth]{figures/domeCalib/39}  
	\caption{Random patterns from 5 projectors are projected on a white tent and captured by all cameras. (Top Left) an HD view; (Top Center) a Kinect view from its color camera; (Top Right) a VGA view; (Bottoms) A visualization of 3D camera localization and a reconstructed 3D point cloud} 
	\label{fig:spatialCalibration}
\end{figure}
\section{Spatial Calibration}
We use Structure from Motion (SfM) to calibrate all of the 521 cameras. To easily generate feature points for SfM, five projectors are also installed on the geodesic dome. For calibration, they project a random pattern on a white structure as shown in the Figure~\ref{fig:spatialCalibration}, and multiple scenes (typically three) are captured by moving the structure within the dome. We perform SfM for each scene separately and perform a bundle adjustment by merging all the matches from each scene. We use the VisualSfM software~\cite{vsfm} with 1 distortion parameter to produce an initial estimate and a set of candidate correspondences, and subsequently run our own bundle adjustment implementation with 5 distortion parameters for the final refinement. The computation time is about 12 hours with 6 scenes (521 images for each) using a 6 core machine. 
In this calibration process, we only use the color cameras of Kinects. We additionally calibrate the transformation between the color and depth sensor for each Kinect with a standard checker board pattern, placing all cameras in alignment within a global coordinate frame.
%and the depth cameras are stitched to the global camera coordinate of other cameras in the end. 

\section{Limitations}
We found two limitations in our hardware design which can be reconsidered for follow-up research. The first is the incompatible frame rates among heterogeneous sensors, especially between HD cameras and VGA cameras, which makes it hard to fuse them for 3D reconstruction. Due to this reason, our method currently uses VGA cameras only, but we expect that this issue can be dealt with by interpolating cues in  the common time domain, because all the sensors are temporally aligned in millisecond level. The second issue is that all the camera views mainly focus on the center of the dome and, thus, fewer views are available at the edges of the capture volume. Such design is ideal given the assumption that subjects are located at the center of the system, but we observe that sometimes people tend to stand near the walls during social interactions. An alternative direction would be to make cameras focus on random locations so that view coverage can be uniformly spread throughout the working volume. 



%\section{Lighting}
%\section{Storage}
%\section{Hardware Details}

%\section{Capture Procedures}

%\section{Data Size}


%We use projectors to generate patterns. 
%We turn on projector one by one to reduce interference among them. 
%We run SfM using all the data. 
%We merge all the matching into a whole optimization. 
%
%With/Without tent
%All projectors/each projectors
%1 tent location, several tent location
%
%We evaluate the accuracy using a single scene with 2D maching points. Using each calibration, we reconstruct 3D and compute reprojection  error. 
%We use leave-out method to check each camera's reprojection error. 
%For kinect, we use checker board pattern. 
%Temporal alignment
%Among VGA
%Among HD
%Among Kinect
%VGA-HD
%VGA-HD-Kinect




\pagebreak